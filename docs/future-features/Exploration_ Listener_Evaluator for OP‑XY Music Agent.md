# Technical Design Exploration: Listener/Evaluator for OP‑XY Music Agent

## Overview and Integration with OP‑XY System

The Listener/Evaluator will be a new component in the OP‑XY “vibe coding” system that analyzes audio output in real-time and provides *per-bar* feedback in JSON form. It acts as an **analysis agent** alongside the existing Conductor (JSON document manager) and Playback Engine (MIDI real-time output)[\[1\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L62-L70)[\[2\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L50-L58). Its role is to “give the music agent an ear” – measuring how well the current loop’s sound fits a target genre and how compatible the patches (sounds) are, then output machine-readable metrics and recommendations.

**Integration with existing architecture:** The Conductor currently broadcasts state (transport/clock), the loop JSON (doc), and basic performance metrics[\[3\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L52-L60)[\[4\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L168-L175). We have two integration options:

* *Option 1:* **Extend Conductor metrics** – Incorporate the Listener’s analysis output into the metrics WebSocket event or a new analysis event. The vibe-coding LLM agent (Codex) can subscribe to these and get JSON packets of analysis each bar. This aligns with the existing pub-sub model and ensures the LLM always has up-to-date audio metrics along with the loop data. We’d need to extend the Conductor’s WebSocket schema to include fields like loudness, spectral breakdown, genre scores, etc. This keeps architecture simple (single WS channel) but we must ensure adding these fields doesn’t confuse the Web UI or other consumers (we may broadcast analysis only when an agent is connected, or namespace it under an "analysis":{...} sub-object in metrics).

* *Option 2:* **Dedicated Listener agent/process:** Run the Listener as a parallel process or thread that captures audio and computes metrics, then provides results on demand. For example, the vibe agent could call an API (function or REST) like get\_current\_bar\_analysis() or subscribe to a separate channel. The provided pseudo-code in the PRD suggests a direct function call interface (e.g. analyze\_bars(n) \-\> list\[BarPacket\]). In practice, this could be exposed to the LLM agent via a tool or function call. This option decouples the heavy audio DSP from the time-critical Conductor thread, at the cost of more integration work (the LLM agent needs a way to invoke or receive listener output).

We’ll likely pursue a **hybrid**: implement the Listener as a separate module or thread (to not block MIDI scheduling) that continuously captures audio and computes metrics each bar, and then sends the results into the Conductor’s broadcast (or makes them available to the LLM). This keeps Conductor’s real-time guarantees (no DSP in the scheduler thread) while leveraging the existing WS broadcast to deliver analysis to the agent. In AGENTS.md terms, the Listener would be a new agent producing analysis “metrics” for the Vibe Coding Agent[\[5\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L30-L38). We’ll stamp each analysis packet with the corresponding bar number and perhaps the docVersion or a timestamp, so the LLM can associate metrics with the correct loop state.

**JSON output format:** We will adhere closely to the proposed structure in the PRD for bar-level and summary packets (see Section 5 of PRD). Each bar packet will be a compact JSON with short keys, e.g.:

{ "t":"bar", "bar":37, "tempo":124,  
  "mix": {"lufs\_s": \-15.0, "clip\_s": 0.0},  
  "spec": {"lo":0.31, "mid":0.47, "hi":0.22, "centroid\_hz": 3100},  
  "key": {"pitch\_class":"F", "mode":"minor", "conf":0.78},  
  ... etc ...  
}

And every 4 or 8 bars the Listener will emit a summary with trend deltas and top flags, e.g. { "t":"summary", "bars":\[37,40\], "delta":{...}, "top\_flags": \[...\], "recommend": \[...\] }. These JSON schemas will be defined precisely (we can provide a JSON Schema file to lock down format). The design goal is **machine-readable, stable output** – i.e. the LLM will get a consistent small JSON that it can parse without confusion. We’ll ensure keys and structure remain fixed (no free-form text except short “why” strings in recommendations).

This JSON is separate from the loop JSON (opxyloop) to keep concerns separate: the loop JSON describes *what to play*, and the analysis JSON describes *how it sounds*. We will include references (like track IDs) in analysis output to tie metrics to tracks in the loop. For example, the "compat.patch\_fit" object maps the track.id from the loop JSON to a compatibility score[\[6\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/docs/prs/0001-initial-docs.md#L6-L9). The Listener will use the Context input (which includes roles and track IDs/preset info) to know which track is “kick”, “bass”, etc., so it can label metrics like kick\_bass\_overlap. The analysis JSON packets can be logged or versioned similarly to loop changes (we might not Git-track them, but they are ephemeral data for the agent).

In summary, **the Listener sits alongside the Playback Engine**, tapping the audio output and feeding analysis metrics to the LLM agent. It will be implemented in Python (for rapid development and integration with our existing Python-based Conductor/Agent codebase), using efficient DSP libraries under the hood. Next, we break down the technical options for each part of the Listener pipeline, referencing state-of-the-art approaches and open-source tools.

## Audio Capture and Bar Synchronization

**Goal:** Tap the stereo mix output in real time on macOS (Apple Silicon) with minimal latency, and slice the audio into bar-length segments aligned to the musical bars.

**Capture path options:**

* **Virtual loopback device (software):** This is the recommended approach for flexibility. We can use the open-source **BlackHole** driver on macOS to create a virtual audio interface[\[7\]](https://existential.audio/blackhole/#:~:text=BlackHole%20is%20a%20modern%20macOS,applications%20with%20zero%20additional%20latency). The user (or our app installer) would set up an **aggregate device** that sends the system’s audio output (or specifically the OP‑XY device output) into an input channel we can record. BlackHole operates with *zero additional latency* on macOS[\[7\]](https://existential.audio/blackhole/#:~:text=BlackHole%20is%20a%20modern%20macOS,applications%20with%20zero%20additional%20latency), which is ideal for real-time. This means we can capture the exact audio the user hears. The Listener will open this virtual input just like a microphone. (If BlackHole is not already installed, we may prompt the user or include instructions, as it’s free/MIT-licensed.)

* **Physical loopback (hardware cable):** An alternative is connecting the Mac’s audio output to the Scarlett 2i2’s inputs via a short TRS cable. In this case, the Scarlett acts as the input device for capture. This is more cumbersome and introduces an extra D/A and A/D conversion. It’s not our first choice, but we can support it as a fallback. The code would treat it like any other microphone input.

* **Direct input from OP‑XY:** In some setups, the OP‑XY might output audio directly to the Scarlett (if the OP‑XY is an instrument with audio outputs). In that case, we capture that input. This is effectively the same in software terms – we choose the Scarlett as the input device.

We’ll make the input device *configurable* (e.g. a setting to choose BlackHole vs specific interface channels). The start\_listener() API could accept a device name or use a default logic: if a virtual loopback is configured, use that; otherwise default to Scarlett’s input channels.

**Python libraries for audio I/O:** We have a few good, permissive options:

* **sounddevice (MIT license)** – A lightweight Python binding of PortAudio. It allows easy recording from an input stream with callback or blocking reads[\[8\]](https://github.com/spatialaudio/python-sounddevice#:~:text=spatialaudio%2Fpython,see). We can specify the device and use CoreAudio on Mac. sounddevice is simple and reliable for stereo PCM capture.

* **soundcard (BSD-3-Clause)** – A pure Python library by Bastian Bechtold that uses the CoreAudio API via CFFI[\[9\]](https://soundcard.readthedocs.io/en/latest/#:~:text=SoundCard%20is%20licensed%20under%20the,clause%20license). It has a nice high-level API for grabbing the default speaker or mic, and even can get loopback sources on some systems. For example, sc.all\_microphones() and sc.get\_microphone('Scarlett') can get the input, and then mic.record() gives a NumPy array of samples[\[10\]](https://soundcard.readthedocs.io/en/latest/#:~:text=import%20soundcard%20as%20sc)[\[11\]](https://soundcard.readthedocs.io/en/latest/#:~:text=,abs%28data%29%29%2C%20samplerate%3D48000). soundcard can also open the default system output as an input (some systems expose a “.monitor” device). On macOS, the user typically must create the aggregate device manually (BlackHole). Once that’s done, soundcard can fetch it by name.

Both libraries abstract the CoreAudio buffers into NumPy arrays, which suits our use. **Performance:** We only need 48 kHz stereo, which is low throughput for modern systems. Capturing continuously at 48kHz\*2 channels (≈0.768 MBytes/s) is trivial relative to disk or memory speeds. We will run the capture in a separate thread or use asynchronous callbacks so that analysis can happen in parallel or at least won’t starve audio input.

**Synchronization to bars:** We need our analysis windows to align exactly with musical bars. We have the following data to leverage:

* The transport tempo (BPM) and time signature from the host (the OP-XY environment). The Conductor or UI can provide the current bar index and tempo. Indeed, the PRD defines a submit\_transport({bar: X, tempo: Y, time\_sig: "4/4"}) interface. In practice, the Conductor already tracks barBeatTick and has tempo info[\[12\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L168-L176). We can use a callback or message each time a new bar starts.

* The playback engine is driven by MIDI clock (external or internal). In MVP, likely the OP-XY device is sending MIDI Clock and Start/Stop. Conductor counts bars by listening to ticks (24 PPQN pulses) and knows when a bar passes. We can piggyback on that: e.g., the Conductor could emit a lightweight event each bar (or we can simply read bar\_index from the state broadcast).

**Design:** When the Listener starts, it will align on the next bar boundary. For example, if the user hits play and bar 1 starts, we start capturing audio into a buffer corresponding to bar 1\. We know bar length in seconds \= (bar\_length\_beats \* 60 / tempo). For 4/4 at 120 BPM, 1 bar \= 2.0 seconds. We will record that many seconds of audio (with a slight safety margin or using the tick events to know exactly when to cut). If the tempo is dynamic (not likely in v1), we’d adjust window lengths each bar.

To ensure alignment, a simple method is: \- **Option A:** Start a timer at bar start and record a fixed number of frames \= round(bar\_duration\_seconds \* sample\_rate). This relies on accurate system timing and can drift if the clock isn’t perfect. \- **Option B:** Use MIDI tick events – e.g., when 24 PPQN ticks accumulate to a bar (e.g., 96 ticks for a 4/4 bar at quarter-note=24 ticks), trigger the slice. The Conductor can expose a callback or the Listener thread can subscribe to the WS state{barBeatTick} messages which likely include the bar count.

We will likely implement a small **ring buffer** for audio: continuously record audio into a buffer, and each time a bar boundary is reached, we slice the last N samples as one bar’s audio for analysis. This way, we don’t have to stop/start the stream every bar (which could introduce pops or latency). Using sounddevice or soundcard in callback mode, we can continuously fill a cyclic buffer and mark indices for bar splits.

**Error handling:** If the transport is stopped or the device is silent (no audio), the Listener should handle it gracefully: \- If clock/transport is lost mid-session, we may attempt to detect bar starts via audio (e.g., detect a consistent click or beat) but that’s complex. Instead, we’ll rely on the transport data – if it stops, we pause analysis and set a sync\_warn flag in JSON. \- If audio frames drop (buffer underrun/overrun), we note drop\_warn in the JSON, and possibly fill missing samples with silence (or repeat last few ms) to keep analysis continuous[\[13\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L18-L22).

**Latency and overhead:** Both sounddevice and soundcard allow specifying buffer sizes. We don’t need ultra-low latency (a few 100 ms latency is fine for analysis as long as bar alignment is correct). To reduce CPU, we might record in **mono** for feature extraction (summing L+R) except where stereo is needed. Indeed, the PRD suggests capturing stereo but doing many features on a mono-summed signal for speed (and using stereo data only for width measurements). We can downmix to mono on the fly (just average L and R samples) for all features except stereo width. This halves FFT workload. Given our 15% CPU budget, we can comfortably do an FFT on 2 seconds of audio and various calculations within \~50 ms in Python with vectorized libs (more on that in the Features section).

In summary, **preferred setup:** Use BlackHole (zero-latency loopback) to route the OP-XY audio to our Listener. Use python-sounddevice (MIT) or soundcard (BSD) to capture 48 kHz stereo into NumPy arrays. Sync to bars via Conductor’s known tempo/bar info. Implement a ring buffer or callback to continuously stream, slicing on bar boundaries. This approach is cross-platform (with PortAudio, Windows WASAPI loopback could be used in the future, though our focus is Mac).

We will document to the user how to configure BlackHole or physical loopback if needed, but we aim to make it “just work” with minimal setup (possibly by shipping a config for BlackHole aggregate device or detecting it automatically by name).

## Loudness Normalization (LUFS)

Before comparing how things *sound*, we need to ensure loudness differences don’t bias the evaluation. The Listener will perform **loudness measurement and normalization** on each captured bar segment.

**Loudness measurement:** We will use LUFS (Loudness Units Full Scale) as defined by ITU-R BS.1770-4 and EBU R-128 standards, which approximates human perceived loudness by frequency weighting and gating silence[\[14\]](https://www.eecs.qmul.ac.uk/~josh/documents/2021/21076.pdf#:~:text=%5BPDF%5D%20Convention%20Paper%2010483%20,is%20both%20fully%20compliant)[\[15\]](https://www.mdpi.com/1999-4893/17/6/228#:~:text=Automated%20Personalized%20Loudness%20Control%20for,both%20human%20perception%20and). Specifically, we’ll measure: \- **Integrated LUFS** over the bar (momentary loudness integrated with gating of parts below a threshold). \- **Short-term LUFS** (e.g. 3-second window as per R-128) at the bar’s length – effectively this might equal integrated for a single bar, but if we ever analyze longer windows we might output both.

The open-source **pyloudnorm** library (MIT licensed) is a great choice here. It provides a pyln.Meter implementing BS.1770[\[16\]](https://github.com/csteinmetz1/pyloudnorm#:~:text=Implementation%20of%20ITU,weighting%20filters%20for%20additional%20control) and functions to measure integrated loudness and even do loudness normalization on a buffer[\[17\]](https://github.com/csteinmetz1/pyloudnorm#:~:text=,integrated_loudness%28data). We can create a meter for 48 kHz stereo and simply call integrated\_loudness(data) on our NumPy array[\[18\]](https://github.com/csteinmetz1/pyloudnorm#:~:text=import%20soundfile%20as%20sf%20import,pyloudnorm%20as%20pyln). This returns LUFS. We can also get short-term loudness if needed by analyzing smaller sub-windows or using meter.loudness\_segment(data).

We should also measure **True Peak** or at least sample peak to detect clipping. “clip\_s” in the JSON presumably is the short-term clip indicator – the PRD shows clip\_s: 0.0 which likely means 0 dBFS (no clipping headroom used) or could mean “0 samples clipped”. We need to clarify: \- It could be **peak dBFS** of the segment (e.g., 0.0 dB if at full scale). \- Or a count/duration of clips.

Given context, it might be peak level relative to 0\. So if clip\_s \= 0.0 it might mean no clipping (peak exactly at 0 dBFS or no over). If a positive value like 1.5, maybe 1.5 dB over 0 (clip). However, digital clipping doesn’t really go above 0 dBFS unless we measure after normalization. Possibly they mean “samples clipped (short-term) \= 0.0%”. We might interpret it as fraction of time or samples that were clamped. We can easily compute sample peak. We’ll include: \- mix.lufs\_i (integrated LUFS over bar, gated). \- mix.lufs\_s (short-term LUFS, perhaps similar here). \- mix.clip\_db or clip\_peak or similar – indicating if any clipping occurred. Alternatively, we can mark a boolean flag if peak is 0 dBFS.

The **normalization step** (if any) would be to level-match to a reference (maybe \-14 LUFS integrated, a common streaming loudness). The PRD implies we do loudness match “for A/B fairness”. This suggests if we audition multiple patches or arrangements, we should loudness-normalize them before comparing genre fit scores so that louder doesn’t falsely seem “better”. For our analysis, we can either: \- Compute the loudness of each bar, then adjust the audio (gain) to a reference LUFS (say \-14) before extracting other features. This ensures features like spectral balance aren’t skewed by one version being quieter/louder. \- Or compute features on raw audio but account for loudness in the scoring (less ideal for comparisons).

It’s probably cleaner to actually normalize the audio segment to a fixed LUFS (within reason) before computing genre/compatibility scores. pyloudnorm has normalize.loudness(data, current\_loudness, target\_loudness) for this[\[17\]](https://github.com/csteinmetz1/pyloudnorm#:~:text=,integrated_loudness%28data). We’d be careful to avoid clipping when raising gain – maybe limit the gain if peaks would clip, or apply a slight limiter (though that might change timbre). In practice, since we’re analyzing and not re-playing the normalized audio, we can allow minor clipping in analysis if it’s consistent across versions. Alternatively, we always downscale louder segments rather than boost quiet ones, to avoid introducing artificial clipping.

We will output the measured loudness values regardless, and a flag if normalization was limited or if we had to fall back to RMS (say the segment is too short for true LUFS gating – though 2s should be okay with short-term metric). For example, if a bar is extremely silent and the LUFS algorithm can’t compute integrated (needs gating block), we might revert to basic RMS for that bar and mark norm\_fallback:true in the notes.

**Library performance:** pyloudnorm is pure Python but not too slow – it processes frame-by-frame with filters. However, our bar is short; even a Python implementation will be fine (and we can reuse the same meter object). If needed, we can try the C-based ebur128 library (there’s ebur128 Python binding, GPL licensed though) – probably overkill.

**Outcome:** Each bar’s JSON will include something like:

"mix": {  
  "lufs\_i": \-14.5,   // integrated LUFS  
  "lufs\_s": \-15.0,   // short-term LUFS (momentary loudness for that bar)  
  "tp\_max": \-1.0     // true-peak in dBFS (or sample peak)  
}

And perhaps clip flags if needed. We ensure both channels are considered (BS.1770 uses a specific weighting for channel summation). These loudness values let the agent do fair A/B comparisons (e.g., if patch A yields \-12 LUFS and patch B yields \-16 LUFS, we’ve normalized internally so genreFit scoring isn’t biased, but we still report those values so the agent knows patch A was louder originally, which might indicate it’s using more compression or such).

In summary, **we’ll use pyloudnorm (MIT) for LUFS**[\[19\]](https://github.com/csteinmetz1/pyloudnorm#:~:text=License) to measure loudness and normalize audio for subsequent feature extraction. This ensures reliable loudness matching per bar, fulfilling the success criterion of “loudness-matched A/B” analysis.

## Feature Extraction per Bar (Core Audio Features)

Once we have a bar of audio (captured and loudness-normalized), we extract a bundle of audio features that are **fast to compute** yet informative about spectrum, harmony, rhythm, and timbre. We will leverage both well-known DSP techniques and recent MIR (Music Information Retrieval) research to implement these.

To meet the real-time budget, we’ll compute an STFT (Short-Time Fourier Transform) once and derive multiple features from it. We’ll also use efficient numpy-based computations or optimized libraries (librosa, etc.), avoiding Python loops. The target is **≤25 ms** per bar for feature computation (the PRD’s goal) – which is achievable on M1 by using vectorized operations and C/C++ backed libraries for heavy lifting.

Below we outline each primary feature and the options to implement it:

### 1\. Spectral Distribution (Low/Mid/High energy & Spectral Centroid)

**What & Why:** We want to quantify how the energy of the music is distributed across frequency bands. Genres differ in spectral profile (e.g., EDM “House” has strong lows and airy highs, Boom-bap hip-hop might be mid-heavy but rolled-off highs, etc.). Also, a good arrangement often balances frequencies (no critical over-/under-filled bands). We will compute: \- **Band energy fractions:** likely 3 bands: Low, Mid, High. For example, the PRD’s sample shows lo:0.31, mid:0.47, hi:0.22 meaning 31% of the energy is low-frequency, 47% mid, 22% high. We need to define band cutoffs. A sensible choice: **Low** \= up to \~250 Hz, **Mid** \= 250 Hz–4 kHz, **High** \= above 4 kHz (this roughly separates bass/sub, midrange, and treble “air”). These can be tuned (we might use 300 Hz and 5 kHz as cutoffs, for instance, or even genre-dependent cutoffs for more nuance).

To get these, simplest method: apply digital filters (e.g., biquad low-pass at 250 Hz to get low content, etc.) and compute RMS energy. But filtering in Python per bar might be slower than doing it in frequency domain. Since we will have the magnitude spectrum from STFT, we can just sum the power in the FFT bins corresponding to those frequency ranges: \- Identify bin indices for 250 Hz and 4 kHz given FFT size and sr=48k. With n\_fft=2048, bin resolution is \~23 Hz. So sum bins \[0:10\] for low (≈ \<230 Hz) and \[10:\~174\] for mid (≈ 230–4010 Hz), rest for high. We might refine these exact indices. \- Normalize each sum by total power to get fractions. This approach uses the STFT results directly (no extra filtering needed). It’s one vector dot product per band effectively, negligible cost.

* **Spectral Centroid:** This is the “center of mass” of the spectrum[\[20\]](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html#:~:text=Spectral%20Centroid), computed as ∑fPf∑Pf for frequencies f and power P(f). It correlates with the **perceived “brightness”** of the sound[\[21\]](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html#:~:text=,the%20%E2%80%9Cbrightness%E2%80%9D%20of%20a%20sound) – high centroid \= bright/trebly mix, low centroid \= warm/dull mix. We can compute the centroid per frame or overall. Likely we’ll compute an overall centroid for the bar by averaging the per-frame centroid weighted by frame energy (or equivalently computing from the average spectrum). Librosa provides librosa.feature.spectral\_centroid which can give a time series[\[22\]](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html#:~:text=Let%E2%80%99s%20compute%20and%20visualize%20the,spectral%20centroid). We might output just the mean centroid in Hz for the bar. In the example, centroid 3100 Hz indicates moderately bright. We can compare this to genre norms (e.g., ambient might have lower centroid on average than EDM).

We’ll implement this either via librosa (which will use the magnitude spectrogram we already have) or manually with numpy (since it’s a one-liner once we have the spectrum):

centroid \= (freqs \* mag\_spectrum).sum() / mag\_spectrum.sum()

(where mag\_spectrum is the mean magnitude per bin over the bar, or sum of all frames’ magnitudes).

**Open-source:** *Librosa* (BSD-3-Clause) provides easy functions for spectral centroid and can compute band energies via its fft\_frequencies and slicing. Alternatively, *Essentia* (AGPL) also provides these descriptors (Essentia calls spectral centroid “center” and has band energy functions too)[\[23\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=loudness%20,key%20and%20scale%2C%20tuning%20frequency). Given license concerns, we prefer librosa or numpy for these basics – librosa’s implementation is already optimized in C for FFT via numpy’s FFT pack. We’ll cite that spectral centroid is a standard measure for timbre brightness[\[20\]](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html#:~:text=Spectral%20Centroid).

**Expected performance:** Computing an STFT of 2048 FFT size with hop=1024 over 2 seconds at 48k is about 94 frames. FFT of length 2048 is very fast (likely under 0.1ms each on Apple’s Accelerate FFT). So \~94 \* 0.1 \= 9.4ms, plus overhead – quite fine. Summing bands and centroid is O(N\_fft) per frame, negligible. So well within 25ms.

**Academic context:** Using band-wise energy and centroid is a lightweight proxy for spectral shape that research shows correlates with timbral qualities[\[21\]](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html#:~:text=,the%20%E2%80%9Cbrightness%E2%80%9D%20of%20a%20sound) and mix clarity. Hermes (2017) identified the “harmonic centroid” (which is similar to spectral centroid) as a key predictor of spectral clarity in mixes[\[24\]](https://www.arpjournal.com/asarpwp/wp-content/uploads/2021/12/Kirsten-Hermes_ARP2019.pdf#:~:text=As%20established%20in%20a%20prior,are%20established%20in%20this%20paper) – in other words, where the bulk of energy lies can indicate if a mix is muddy (too much mid) or thin (no low). We will leverage these as inputs to scoring (e.g., GenreFit might expect a centroid in a certain range, and ArrangementClarity will reward a balanced lo/mid/hi distribution).

### 2\. Tonal Analysis – Key, Scale, and Harmonic Content

**What & Why:** We want to detect the musical **key (tonal center) and scale (mode)** of the music in each bar, along with a confidence measure. This addresses potential **key clashes** between parts and helps the agent ensure everything is in the intended key. If the user provided a key\_hint (“F minor” in the example context), we expect the music to generally align with that; if our detected key differs or is unstable, that’s a signal of incompatibility.

**Options for key detection:**

* **Chromagram & Template Matching (lightweight):** We can use a **chromagram** (12-bin representation of pitch class energy) computed from the audio. Librosa can generate a chromagram from the STFT or using constant-Q transform. A straightforward approach: sum the magnitudes or energies across all frames for each pitch class (maybe using the harmonic component of the spectrum if available). Then compare this 12-dimensional vector to reference profiles for each key. The classic Krumhansl-Schmuckler method uses predefined profiles for major and minor keys and correlates the song’s chroma histogram with each profile[\[25\]](https://essentia.upf.edu/tutorial_tonal_hpcpkeyscale.html#:~:text=Essentia%20essentia,edma%20%2C%20is%20specifically). The key whose profile fits best is chosen, and the correlation strength can serve as a confidence.

We can implement this: we have chroma \= librosa.feature.chroma\_stft(y), sum over time to get a 12-vector. Then have arrays for major and minor profiles (e.g., major profile emphasizing scale degrees I, III, V, etc.). We circularly rotate the profile for 12 possible root notes and compute dot product. This yields 24 possible matches (12 major, 12 minor). Pick the best. This is O(12\*24) \= trivial. Librosa doesn’t directly give key, but it gives chroma; we’d implement the matching or use music21’s key detection (but music21 works on MIDI mostly and might be heavy).

* **Essentia Key algorithm (robust but AGPL):** Essentia has a built-in Key extractor which does exactly the above (HPCP \-\> key) using profiles tuned for EDM (“EDMA” profiles, which might be suitable for our genres)[\[25\]](https://essentia.upf.edu/tutorial_tonal_hpcpkeyscale.html#:~:text=Essentia%20essentia,edma%20%2C%20is%20specifically)[\[26\]](https://mtg.github.io/essentia.js/docs/api/EssentiaExtractor.html#:~:text=EssentiaExtractor%20,estimation%20using%20the%20Key%20algorithm). It outputs key, scale, and strength. It’s known to work well and also accounts for tunings. However, Essentia is AGPL-licensed[\[27\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=analysis%20and%20audio,classifying%20the%20results%20of%20audio), meaning if we include it, our whole program would need to be AGPL (unless we negotiate a license). If our project is open-source anyway, we might consider it; otherwise, we should avoid linking it directly. We could possibly call Essentia as an external binary (“essentia\_streaming\_extractor”) to avoid linking, but that’s clunky.

Given our licensing preference for MIT/BSD, we’ll likely implement key detection ourselves using librosa \+ custom logic. It’s not too hard and for short loops it should suffice.

* **Confidence measure:** We’ll compute a correlation value or difference between top two matches to determine confidence (0 to 1). For instance, if the best match correlation is 0.85 and next is 0.5, confidence \~0.85 or a scaled difference. Essentia’s strength is similar (it gives a number where 1.0 is very confident). We will output that as conf. A low confidence (say \<0.5) might mean either polytonality (two keys present) or no clear tonal center (e.g., atonal or just drums). That would flag arrangement issues.

* **Handling short segments:** One bar might be very short to get a reliable key (especially if only a couple of chord hits). We might aggregate over 4 bars for key detection to be more stable, but then we lose bar-specific changes. Perhaps do both: per bar key (for detecting sudden changes or ambiguous sections) and overall key for the summary. The PRD suggests the summary will roll up stability of tonal center.

**Chroma implementation detail:** Since our loop JSON knows the meta.key (if provided) and each track’s notes (if using degrees), one might ask: why not use that? The reason is we want to detect *audio* key, which might drift if a patch is tuned differently or if there’s a bass line implying a different root. Using audio ensures we catch any detuning or errors. But we can use the key\_hint as a reference to compare against detected key for scoring (e.g., if hint says F minor but detected is G minor at high confidence, that’s a red flag).

We’ll likely use **librosa**’s constant-Q spectrogram or harmonic pitch class profile approach for robust chroma. The constant-Q transform (CQT) is good for tonal analysis (pitch resolution). Librosa’s chroma\_cqt might give better results than STFT for key, especially for low bass notes. We should also consider filtering out transients (drums) from chroma – drums can muddy chroma with broadband noise. Perhaps emphasize sustained tonal parts by using a longer window or only frames with harmonic content (we could apply a spectral centroid threshold to exclude noisy frames from chroma calculation, or use an HPSS – harmonic/percussive source separation – to isolate harmonic component).

To keep things light, a simpler route: apply a weighting so that strong low-frequency fundamentals count (maybe doubling the amplitude of bins below some freq in computing chroma) – but that might be overkill. We assume tonal content will show in chroma clearly if present.

**Output:** We’ll output:

"key": { "pitch\_class": "F", "mode": "minor", "conf": 0.78 }

If completely unsure, we could output "pitch\_class": null or conf=0. But likely we always output something and rely on conf to indicate uncertainty.

**Academic reference:** The use of HPCP (Harmonic Pitch Class Profile) and key detection is standard MIR. Essentia’s key extractor (combining HPCP and profile matching) is a proven approach[\[26\]](https://mtg.github.io/essentia.js/docs/api/EssentiaExtractor.html#:~:text=EssentiaExtractor%20,estimation%20using%20the%20Key%20algorithm)[\[28\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=complexity%2C%20rolloff%2C%20contrast%2C%20HFC%2C%20inharmonicity,annotations%20based%20on%20SVM%20classifiers). It’s lightweight enough for real-time when bar-length audio is used (especially given we only have a few instruments playing).

### 3\. Onset and Groove Analysis (Timing)

**What & Why:** The Listener should quantify the **groove and timing** – specifically, how tight or humanized the timing is, and the swing. The PRD calls for **onset variance (ms)** and **swing %** per bar. These metrics help detect if parts are off-grid or if swing is present, which affect genre feel (e.g., house often has a slight swing \~55%, certain hip-hop prefers loose “drunk” timing, etc.). If things are unintentionally off, the agent might suggest quantization or if they’re intentionally swung, the agent should recognize that.

**Onset detection:** We can either use the known grid from the loop JSON or detect from audio: \- If the loop JSON pattern is known and the user hasn’t done micro-timing beyond what JSON expresses (the JSON does allow microshiftMs per note), we could theoretically compute expected onset positions. But it gets tricky if the patch has an attack or latency – audio might have slight delays (e.g., a pad with slow attack won’t onset sharply on the beat). For a true “ear” perspective, audio detection is more directly meaningful.

* We will likely **detect onsets in the audio** using a simple method like spectral flux or energy threshold. Open-source:

* **Librosa** has librosa.onset.onset\_detect which uses a spectral flux method under the hood and can return onset times.

* **aubio** (GPL) offers an onset detection function which is very efficient in C (could use “energy” or “complex-domain” onset detectors). However, aubio is GPL-3[\[29\]](https://github.com/aubio/aubio#:~:text=aubio%20is%20free%20software%3A%20you,the%20Free%20Software%20Foundation%2C) – not permissive. Given we can do it with librosa or even roll our own (spectral flux \= difference in consecutive FFT magnitudes, sum positive differences), we’ll avoid aubio to keep licensing clean.

We might do this: compute the onset strength envelope via librosa (which gives a curve indicating likelihood of an onset at each frame). Then find peaks in that curve above a threshold. The output will be frame indices or times of detected attacks.

* **Onset variance:** Once we have onset times, how to get a single “variance (ms)” value? Probably by comparing onsets to the nearest grid points (16th or 8th notes). For each detected onset (especially for percussive elements like kicks, snares, hats), compute its deviation from the exact quantized position (we know tempo, so if an onset is at 1.05 beats, that’s 0.05 beat late, at 120 BPM that’s 25 ms). Compute the standard deviation of these deviations. A smaller value \= very tight timing; a larger \= more human variation. We might restrict to certain instruments (maybe primarily the drum hits) to compute this, since a pad with a slow attack will look “late” but that’s intentional envelope, not rhythm misalignment. One heuristic: only consider onsets that correspond to drum transient frequencies (like significant energy in high-frequency for a hat/snare or low-frequency for a kick). Alternatively, because we know roles, we might specifically track kick and snare onsets – e.g., detect low-freq onset for kick, mid for snare. But that complicates detection.

Simpler: detect all onsets, then the variance of spacing or alignment. Possibly the PRD expects this to reflect “groove looseness”. We can do: take the differences between actual onset times and a perfect grid (the grid might be 1/16 note increments if stepsPerBar=16). If the loop is quantized, this variance \~0 (except maybe swing offsets). If the loop has microshifts, variance \>0.

* **Swing %:** Swing is the purposeful delay of every second (and fourth, etc.) subdivided beat. In a 4/4 with 16 steps, swing usually means the 8th-note offbeats are delayed. If swing is applied by our system, the JSON meta likely has a swing value (0–1) that the playback engine already uses. The agent could simply copy that. However, the PRD says “(grid or detected)”, meaning if available from the data we can use it; if not, we should detect it from the timing of onsets.

To detect swing from audio: look at the pattern of onset intervals. For instance, at 120 BPM 4/4, the 16th note duration is 125 ms. If swing \= 55%, then the first 16th of a pair is 125ms \* 0.55 \= \~68.75ms, second is 125ms\*1.45 \= \~181.25ms (since pair sums to a quarter note 250ms). So the offbeat (second 16th of each pair) is delayed. In practice, easier: measure the ratio of the duration of an odd vs even 16th. If we detect hi-hat hits on every 8th note, we can measure the difference between consecutive pairs (the time from beat 1 to offbeat, vs offbeat to next beat). That ratio – when expressed as a percentage – is swing. E.g., 0.55 (or 55%) as above.

We could specifically target the high-hat or percussion track for swing detection (hats often carry swing timing). If multiple tracks swing together, any regularly occurring pattern will reflect it.

If we have the JSON and the user set swing in meta, we can trust that and output it (since the engine will be doing it). If the user manually offset notes, detection is needed. We might implement a simple approach: \- Compute auto-correlation of onset time sequences or onset strength at lag \= half-beat. If swing, alternate intervals will differ. \- Or explicitly cluster the short vs long intervals: find all adjacent 8th note intervals from onset list, see if they alternate between a shorter and longer value. If yes, compute swing \= short/(short+long).

This is a bit complex but doable. Since we know the tempo, we know the ideal 8th note length. We can match detected onset times to nearest 8th grid positions and see if the offbeat ones are consistently late by X ms. That lateness relative to a straight grid gives swing percentage (X divided by the ideal offset difference at 50%).

We will likely output groove: { onset\_var\_ms: 7.2, swing\_pct: 55.0 } as in the example. If no swing detected, swing\_pct \~50.0 (meaning straight). If things are quantized, onset\_var \~0. If purely human drums, onset\_var might be say 15 ms and swing could be something like 53% (light swing) or just 50 if no pattern.

**Tools performance:** Onset detection via spectral flux is light (especially for a 2s window). Librosa’s onset detection might take a few milliseconds in numpy. Even if we did a simple peak picking manually, it’s fine.

**Alternate approach (not in v1):** Because we have the **actual MIDI pattern** in the loop JSON, an advanced strategy could be: use the known note on times from the loop to exactly identify intended onset positions, then measure the audio’s actual onset. But since our playback engine is sample-accurate (no delays except those intentionally in microshift or due to envelope), presumably the actual audio onset differences come from sound design (attack times) or intentional microshifts which the JSON already has. So for v1, a simpler audio analysis is acceptable.

**Conclusion:** We’ll implement audio-based onset detection (librosa) to get a sense of timing looseness and swing. We’ll cross-verify it with known swing from the JSON if available. This ensures the Listener can detect if, say, the user or agent loaded a drum loop that is slightly behind the beat or a shuffle rhythm, and quantify it for GenreFit (some genres expect a certain swing range or tightness).

### 4\. Masking and Patch Interaction Proxies (Frequency Overlaps)

**What & Why:** A crucial part of “does it sound good together” is whether instruments are masking each other – e.g. kick and bass clashing in low frequencies, or a lead synth and pad fighting in the same midrange space. True masking is complex (psychoacoustic phenomenon)[\[30\]](https://www.izotope.com/en/learn/what-is-frequency-masking?srsltid=AfmBOoqF27633mIFtuAuALAhpmzAgtqwIB5yZVXiIwNIN2FU4PDGW0cl#:~:text=What%20Is%20Frequency%20Masking%3F%20,in%20the%20same%20general%20location), but we can define **overlap metrics** as proxies: essentially, measure how much two instruments are active in the same frequency band at the same time.

The PRD specifically calls for metrics like kick\_bass\_overlap and lead\_pad\_overlap. These are between specific role pairs of interest: \- Kick vs Bass (low-frequency overlap) \- Lead vs Pad (mid-frequency, and possibly stereo overlap) \- Hats vs Snare (high-frequency percussion overlap)

**Challenges:** We only have the combined mix audio, not isolated tracks (unless we were to solo tracks, which is not feasible in real-time). So how to gauge overlap of, say, kick and bass from just mix? We have a few approaches: \- Use filtering and onset timing to isolate contributions. For example, to estimate **kick vs bass**: apply a low-pass or band-pass around the kick’s fundamental (\~50-100 Hz) and detect transients (kicks are usually short and strong in that range). Apply a band-pass around typical bass range (say 50-250 Hz but excluding the very thumpy sub maybe) and perhaps slightly longer window. Then measure how often they coincide. If every time a kick hits, there’s also bass energy (like a sustained bass note over it), that indicates overlap. If the bass ducks or is silent on kick hits, overlap is low. A crude metric: correlation between the low-frequency waveform of the kick band and the bass band.

Another simpler metric: **Low-frequency energy share** – what fraction of low-frequency energy is from kick vs bass? Without separation, one trick: kicks usually manifest as short bursts of energy. Bass (sustained) manifests as more continuous low energy. We could compute the *variance* of the low-band energy envelope. If it’s highly pulsing (peaky), likely the kick dominates. If it’s more constant (with less contrast between kick hits and gaps), bass is filling the gaps → possibly masking. We can quantify overlap as the “baseline” low-level in between kick peaks. E.g., normalize such that kick peaks \= 1, then measure the energy right before the kick hit – if that is high, the bass was still loud when kick hit (overlap), if it’s near zero, bass was silent or ducked (no overlap). This requires knowing where kicks occur in time (we can detect kick onsets using a low-frequency onset detection).

We have the advantage of the **role info**: the Context roles tell us which track is kick and which is bass. Possibly, we can extract the *MIDI pattern* of kick and bass from the loop JSON (since the loop is known). If we know “kick track has hits at steps X, bass has notes covering beats Y-Z”, we could infer overlap in pattern. But the audio level matters too (maybe a bass note is playing but at a very low volume or filtered out – pattern alone won’t tell that). However, combining pattern knowledge with audio could be powerful: e.g., identify time segments where bass and kick notes coincide and then measure frequency content.

For v1, a simpler audio-driven proxy might suffice. We might do: \- Filter audio with a band (e.g., a band-pass 40-120Hz to capture kick fundamental range). \- Also a band for bass fundamental (maybe 40-300Hz). \- Compute short-term RMS in these bands per 10ms frame. \- Compute overlap \= the average of (min(kick\_env, bass\_env) / max(kick\_env, bass\_env)) or something during times when either is active. Or correlation coefficient between the two envelopes could indicate how often they rise together. If correlation is high, they often coincide (which might actually mean layering, hence potential masking). If one goes up while the other stays low, correlation low, meaning one yields to the other.

This approach is heuristic. We might refine after some testing on actual loops. **We’ll output overlap as a number 0 to 1** (1 \= heavy overlap/conflict, 0 \= totally separated).

* **Lead vs Pad overlap:** Typically in mid frequencies (\~300 Hz–5 kHz) and possibly stereo image. A pad often spreads across stereo; a lead might be centered but if both have similar frequency content, they can mask. We can attempt:

* Compute a mid-frequency band energy for when lead plays vs when pad plays. If both are concurrently producing a lot of mid energy, overlap high. But again, from full mix audio, we can’t tell which portion of mid energy is lead vs pad.

* If one is stereo wide and the other is narrow, one idea: the pad might have a lot of *side channel* energy (if it’s a wide lush pad), whereas the lead might be mostly *mid channel* (if centered). We can separate the mix into Mid and Side signals (M \= L+R, S \= L-R). Perhaps the pad’s presence will reflect as elevated side energy in mid frequencies, and lead as elevated mid (center) energy in mid freq. Then overlap might be inferred if both mid and side in that band are saturated. But that’s tenuous.

More straightforward: use knowledge of their melodic activity. Possibly the loop JSON can tell when the lead is playing (notes on) vs pad chords on. If they constantly play simultaneously, likely some overlap. If pad only plays when lead rests, overlap is low. So pattern timing might actually help here more than audio.

Since we do have the **PatchIDs and roles** and presumably can map them to track patterns, we might integrate a bit of symbolic analysis: \- We know the bars (bar 37 etc) and can inspect the loop JSON for that bar’s events (the Conductor can supply the current doc). If the doc has a track with role “lead” and another “pad”, we can see if in bar 37 the lead has notes at same time as pad chords. If so, mark overlap potential. \- Then use audio evidence: maybe measure the combined spectral flux in the mid band – if both start notes at the same time, there will be a big spectral change (could be one or both though).

Given time constraints, a heuristic audio metric: measure the **spectral flatness or congestion** in mid frequencies when multiple instruments play. If lead+pad overlap, the mid spectrum might be denser (flatter) because two different sounds sum. If only one plays, spectrum might have more distinct peaks (like one instrument’s formant structure). This is speculative, but spectral flatness in the mid band could increase when overlapping, indicating more noise-like combined spectrum (could be perceived as mud). We do plan a roughness/harshness measure (coming later) which is related.

For v1, we might implement a **simpler approach using time-frequency masking:** e.g., calculate the spectrogram and then attempt to factor it into two components when lead or pad are solo vs together. That’s like source separation – too heavy to do perfectly, but maybe we can cheat since we have their note info: if lead is playing higher notes and pad is lower chords, their energy might occupy different subbands or pitch regions. We could integrate over partial frequency ranges (like up to a certain Hz that pad covers vs above that lead covers). But if they overlap in freq, that doesn’t hold.

Given complexity, we likely do a basic measure: \- Compute band-limited energy for the frequency range both occupy (e.g., 300–3000 Hz). \- Look at the RMS amplitude curve in that band. If both instruments weren’t overlapping, that curve might have dips when one instrument stops (if the other isn’t filling). If always high, suggests continuous sound (overlap). \- Possibly detect amplitude modulation: if two sounds overlap, you might see a more steady combined amplitude (since even if one has a slight decay, the other might fill in). If they alternate, the amplitude might fluctuate more. So maybe lower variance of the mid-band envelope \= more constant presence \= likely both layering.

We’ll refine after some experimentation, but these overlaps will feed the **Compatibility scoring** later. We output them as decimals. Example lead\_pad\_overlap:0.34 from PRD suggests moderate overlap 34%. The agent would likely flag if above some threshold (like \>0.5 \= “they’re masking each other”).

* **Hats vs Snare overlap:** This is less critical but included for completeness. High-frequency percussion – if both hi-hats and snare occupy similar spectrum (e.g., a bright snare overlapping open-hat tails), it can cause clutter. We can measure the high-band (e.g. \>5 kHz) and see if whenever snare hits, the hat is also hitting loud. If yes, might suggest the snare’s crack is masked by the hat. Metric: correlation of snare onset times and high-band energy (hats energy). But given many EDM hats run 16ths throughout, maybe not an issue. We might implement it similarly to kick-bass: detect snare transients (snare is mid-freq \~150–250Hz body plus broad noise to 10k, but maybe we detect via mid-freq onset at snare times) and see if hat was active.

**Potential use of source separation:** In academia, one would approach this with **source separation** (like demix the drum track or isolate instruments and then measure overlaps exactly). There are deep learning models (e.g., Demucs) that can separate drums, bass, etc., and we could then directly measure, say, the spectral overlap of kick vs bass by looking at separated stems. However, running a model like Demucs on each bar (even a 4-second clip) on CPU is far beyond 200ms (likely several seconds). Even if optimized on an M1 GPU, it’s heavy and introduces a dependency on a large model (Demucs is MIT licensed but \~120MB model and uses PyTorch). That’s not feasible for our real-time v1, though it’s an intriguing future direction (perhaps a smaller model could be trained for just kick/bass separation).

So for now, we rely on **spectral heuristics and known note timing** to approximate masking. We will carefully tune these with test loops to ensure they correlate with audible clashes. If we find difficulty, we might simplify the first version to only look at *coinciding note events* (from the loop JSON): e.g., if kick and bass have \>=X% of their notes overlapping in time, set kick\_bass\_overlap high. This at least uses perfect knowledge of the composition, albeit not accounting for sound timbre. It might be acceptable as a first proxy (and is very fast). In fact, since our system does have all MIDI patterns, this might be the most reliable initial measure: \- Count number of bass notes that occur within ± some small window of a kick hit (or sustain through a kick hit). \- Divide by total kick hits or total bass notes to get an overlap ratio.

We should clarify in design: is the Listener supposed to treat the music as a *black box* only hearing audio, or can it “peek” at the loop data? The PRD framing (“ear”) suggests primarily audio, but since this is all within our system, there’s no rule against using the loop JSON to aid analysis. A real human listener wouldn’t know the patterns explicitly, but our agent can. To maximize reliability, we’ll **combine audio and data-driven approaches**: for example, confirm a masking issue by both pattern overlap and spectral evidence. This can reduce false positives (e.g., maybe the kick and bass overlap in time but if the bass patch has a high-pass filter, they might not actually mask; pattern alone would flag, but spectral would see bass has little sub content so it’s fine).

**Implementation plan:** \- For *kick\_bass\_overlap*: use pattern overlap ratio as primary metric, adjust by low-frequency spectral content of bass. E.g., if bass has significant energy \<100Hz (we can detect bass’s spectrum – maybe by looking at the bar’s spectrum and seeing if there’s a distinct bass tone peak vs just kick thump). If bass is very sub-heavy and overlaps in time with kick often, mark high overlap. If bass is more mid-bass and maybe not conflicting frequency (like a bass guitar with fundamental at 110Hz vs kick at 60Hz – still some overlap though), moderate. \- For *lead\_pad\_overlap*: use note overlap (does lead play concurrently with pad chords?) and also spectral profile (if both are in similar registers). If the pad is a high pad and lead is also high, more overlap than if pad is an octave down filling low-mid. We could use detected pitch: if we can roughly estimate the dominant pitch of pad vs lead (maybe from chroma or by isolating fundamental frequencies via filters around the note frequencies), but that might be heavy. Simpler: track roles: if pad’s role often covers chords around middle C and lead an octave above, maybe minimal overlap. Hard to automate; perhaps assume moderate overlap if any concurrent notes.

* For *hats\_snare\_overlap*: use time overlap (do hats hit on the downbeats with snares? Many patterns have a closed hat on every 16th including where snare lands, causing that “both at same time”). If yes, and if the hat is loud, might suggest turning hat off on snares to let snare shine (common mixing advice). So we can flag if at snare hit times, the high-frequency content doesn’t drop. This we can do by pattern: check if hat step coincides with snare step often. Or audio: measure high-freq energy drop (if a snare’s energy is mostly mid and hat is high, ideally high band might dip when hat stops for snare – if not dipped, hat likely played).

Given time, likely pattern-based overlap detection is simplest and robust (since we have precise knowledge of sequencing). We will implement that and cross-check with audio amplitude to avoid obvious mis-predictions (like if pattern says overlap but the actual patch is such that it doesn’t matter).

**Output:** Overlap metrics will be 0–1 floats. Possibly we don’t need extremely high precision; two decimals is fine (e.g., 0.34 as in example). They will be included in the "mask" object of JSON. We’ll include only a few key pairs to keep it small.

### 5\. Stereo Width Measures

**What & Why:** Stereo imaging is a part of arrangement/mix quality. We want to measure how wide the mix is overall, and how wide certain elements are, because genre conventions differ (e.g., ambient might have very wide pads, but a mono kick; some genres prefer narrower mixes for punch). The PRD suggests “global width” and per-role width (like lead\_width).

**Global stereo width:** A common metric is the **Mid-Side ratio** or the **stereo cross-correlation**. One approach: convert to Mid (M \= L+R) and Side (S \= L-R) signals. Compute the RMS of each. The wider the stereo image (i.e., more difference between L and R), the higher the Side energy relative to Mid. If a track is purely mono identical L=R, S will be zero (width=0). If it’s fully stereo with L and R completely uncorrelated, side could be as high as mid. We can define width \= **Side / (Mid+Side)** energy fraction. In the JSON example, "stereo":{"width":0.42} likely means 42% of energy is in the side channel – moderately wide.

We’ll implement this easily: after loudness normalization (summing to mono for some features doesn’t matter for width – we should use the original stereo channels for width). Compute mid \= (L+R)/√2, side \= (L-R)/√2 (the √2 keeps energy sum consistent). Then energy\_mid \= mean(mid^2), energy\_side \= mean(side^2) over the bar. Width metric \= energy\_side / (energy\_mid \+ energy\_side). Alternatively, some define stereo width in degrees, but we’ll just output 0–1 fraction. If needed we can output correlation (where 0 \= fully wide uncorrelated, 1 \= mono identical, negative if out of phase; but negative correlation is rare unless deliberately phase-inverted, which we likely don’t encounter often from OP-XY patches).

**Per-role width:** Now, measuring width per instrument is tricky with only mix audio. We cannot directly isolate the instrument’s stereo spread unless we had separate stems or panning info. However, we *do* know some things: \- If a patch is a stereo synth or effect (like a chorus pad), its output likely has a wide stereo spread. If another is a mono sample played dead center, that’s narrow. \- The OP-XY **engine type** might hint: e.g., sampler might often be mono or stereo depending on sample, dissolve or others might have inherent stereo. But unless we have metadata about patch stereo, not straightforward. \- Another approach: if the lead is mostly in a certain frequency band, we can measure the side energy in that band. For example, if the lead occupies 1–2 kHz primarily (we can guess if it’s a certain synth patch), measure side vs mid in 1–2 kHz band. If side fraction is low there, the lead is likely centered; if high, lead is spread or panned. Similarly for pad (often broader frequency range though).

Perhaps a simpler hack: The agent can trial a measurement by temporarily soloing the track and measuring (but that’s not real-time friendly and requires controlling playback differently – not ideal on the fly).

**Proposed solution:** Use overall stereo distribution plus role frequency separation: \- We know roughly each role’s freq range (kick: low, bass: low-mid, pad: mid-high, hats: high, etc.). We can compute the side energy in those sub-bands. For example, measure side/total ratio for \>5 kHz (that’s mostly hats/cymbals) – call that hats\_width proxy. For mid-range (say 500–2000 Hz) which might be vocals/leads/pads, but that’s broad. Alternatively, if lead is melodic in upper mids and pad is more in lower mids, we could split the mid band further by frequency (not exact by role, but might approximate). \- Another idea: if the loop JSON has panning or uses stereo effects, maybe the device (OP-XY) doesn’t expose that; likely not in JSON (the JSON doesn’t have pan parameter per step). The OP-XY engines might have inherent stereo though (like reverb sends etc.)

Given the difficulty, for v1 we might output **global width only** and one or two indicative ones if we can (maybe just lead\_width as in example). The PRD did show lead\_width:0.36 which presumably is side fraction of lead. How to get 0.36? Possibly they assumed lead is somewhat narrower than whole mix.

We can attempt a rough **lead isolation** by focusing on moments the lead plays *alone* vs with pad. If there’s a moment where only lead is active (no pad or other instruments), the stereo content in that moment is attributable to lead. If in that moment the side level is X, that’s the lead’s width. Similarly for pad (maybe measure when lead is silent but pad is playing). This requires note event timing – again using loop data: find a segment (could be a portion of the bar or a whole bar if tracks take turns). If such isolation exists, we can compute that instrument’s width. If not (they always play together), we can’t easily separate. But maybe the system can try a high-pass or band-pass to isolate lead’s notes (if lead is higher pitch than pad, a high-pass above pad’s range might isolate it a bit). If lead is a high melody (say \>1 kHz) and pad a lush lower chord, filtering might separate them partially. We could for instance measure stereo width above a certain cutoff that likely corresponds to lead’s presence. This is guessy but perhaps okay.

To avoid too much guesswork, we might initially provide just stereo.width (global) in JSON. The example did show lead\_width, implying they intend at least that one. If we can’t reliably calculate it, we might output an estimate or leave it and note as future.

**Conclusion:** \- Compute **global width** via mid/side ratio[\[31\]](https://soundcard.readthedocs.io/en/latest/#:~:text=,get_microphone%28%27FS2i2). \- For **lead\_width** (if required): attempt to measure side ratio in the frequency band of the lead’s fundamental content. E.g., if the lead synth is likely occupying, say, \>500 Hz region (assuming pad might cover more full range though). Alternatively, any **melodic solo** instrument might not have much sub-bass, so measure width of everything above, say, 600 Hz. This could attribute pad and lead together though. Perhaps use an even higher cutoff, e.g. \>2 kHz, assuming lead has bright components there whereas pad might be smoother. This is speculative. We can refine by testing known scenarios and seeing if that correlates. If not satisfying, we may drop per-track width from v1 and only output global.

We will clearly flag in design that *detailed per-track width is hard without isolation*, and mark it as a stretch goal. The PRD’s example suggests some attempt though, so we’ll do our best with heuristics.

### 6\. Roughness and Harshness (Timbre Quality)

**What & Why:** “Roughness” here refers to unpleasant dissonance or distortion in the sound. “Harshness” often specifically refers to high-frequency roughness (like too much 5–10 kHz causing a brittle, piercing quality). We want to cap these for mix safety (no extremely harsh sound) and also identify if the mix is too smooth vs too gritty for the genre (some genres like a bit of saturation/roughness, others like clean).

In audio analysis, related concepts: \- **Spectral Flatness (Tonality):** A measure of how noise-like a spectrum is (flat spectrum \= 1.0, pure tone \= 0.0). It’s basically the ratio of geometric mean to arithmetic mean of the power spectrum[\[23\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=loudness%20,key%20and%20scale%2C%20tuning%20frequency). A high flatness means the sound has no strong tonal peaks (could be noise or heavily distorted). We can use spectral flatness in specific bands: \- *Global flatness:* flatness of full-band spectrum indicates overall “noisiness” vs “tonality” of the mix. A drum loop with dense noise (cymbals) will have higher flatness than a pure sine wave bass. \- *High-band flatness:* flatness of, say, 5 kHz–20 kHz region. This could capture harshness – e.g., white noise (very harsh) has flatness \~1, whereas a smooth hi-hat with clear pitch might have lower flatness. \- **Inharmonicity/Dissonance:** Essentia provides a “dissonance” metric which computes roughness based on beating between spectral peaks (Sethares’ model)[\[32\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=zero,annotations%20based%20on%20SVM%20classifiers). That’s a psychoacoustic measure of sensory dissonance (rapid amplitude modulation in the 20-150 Hz modulation range due to close frequency components). This is more detailed, and computing it requires identifying spectral peaks and summing roughness contributions – which can be done but might be slower in Python. Essentia’s implementation is C++ optimized. If we had Essentia available, we might call its **SpectralDissonance** algorithm on the frame spectra (or averaged). But without it, we might approximate by spectral flatness plus checking if any prominent beating frequencies appear (e.g., amplitude modulation components in 200-300 Hz region of the spectrum might indicate roughness, but that’s too advanced).

For v1, a pragmatic plan: \- Compute **spectral flatness** in two ranges: full band, and high band. Librosa has librosa.feature.spectral\_flatness which returns an array per frame. We can average it over the bar. That gives a 0–1 value. We invert it to get a “tonalness” if needed. But roughness we consider \~ high flatness. \- Additionally, measure **high-frequency energy vs mid**: sometimes a sound is harsh simply because there’s too much 8kHz+, even if not flat. So also consider the proportion of energy in 8-12 kHz (“air” band). If it’s extremely high relative to mid, it might be overly bright (harsh). Actually, that might be fine (air isn’t harsh, it’s just bright). The harsh range often cited is \~2–6 kHz (where the ear is sensitive). We could specifically measure energy in 2–6 kHz and see if it’s dominating. If spectral centroid is extremely high, that indicates brightness (which could be harsh if unbalanced).

So maybe: \- **rough.global** \= spectral flatness of 200 Hz–20 kHz (exclude sub-bass as that can artificially increase flatness if silence in high end). \- **rough.hi** (or harshness) \= spectral flatness in, say, 5–10 kHz band OR the ratio of energy in 5–10 kHz to total. We might combine flatness and energy: e.g., if flatness in hi band \> 0.5 and hi band energy fraction is above some threshold, then “harsh\_hi” is high. If hi energy is high but flatness low (means high end is strong but tonal, maybe like a hiss or whistling tone? Actually tonal high could be a sine wave – not harsh but piercing possibly). Flatness captures if it’s noise-like. A narrow resonant peak (like microphone feedback at 5k) would be tonal (flatness low) but still unpleasant. So flatness alone isn’t a full harshness indicator. We might need to also catch if any **narrow peaks in the 2–6k region** are extremely loud (resonances). That could be done by looking at spectral contrast or slopes.

This can get deep; for now, focusing on simpler: \- Use **flatness as a general roughness** metric: e.g., global flatness 0.21 (as in example rough.global:0.21). That’s fairly low, meaning overall sound has good tonal content (0 would be pure tone, 1 would be white noise). \- For harshness specifically, maybe we use **spectral centroid or rolloff** as a proxy plus flatness. If centroid is high (\>5k) and flatness also moderately high, that suggests a lot of high noisy content \= harsh. If centroid is high but flatness low (maybe a very bright but tonal sound, like a high-pitched sine lead), that might be bright but not “rough” – though some might still call it piercing. Hard edge cases.

We might output:

"rough": { "global": 0.21, "hi\_band": 0.17 }

where hi\_band is maybe the flatness in the high band. Lower numbers are smoother. If above a threshold, we could flag “harsh\_hi” in summary flags.

We should test with known scenarios (e.g., white noise hi-hat vs a clean ride cymbal to see how metrics differ).

**Academic context:** *Spectral flatness* is a widely used descriptor for perceptual tone/noise balance[\[21\]](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html#:~:text=,the%20%E2%80%9Cbrightness%E2%80%9D%20of%20a%20sound). Essentia’s high-level descriptors include “inharmonicity” and “dissonance” which are more advanced, but flatness gets at similar concept quickly[\[23\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=loudness%20,key%20and%20scale%2C%20tuning%20frequency). Roughness in psychoacoustics (as per Sethares) would require more heavy computation (peak pair evaluation) – not feasible in 200ms in Python unless using Essentia’s C++ (AGPL) or possibly numba to speed up. We will note that as a possible later improvement (e.g., if we integrate Essentia in the future, we can replace our flatness with its dissonance calculation for more accuracy in measuring chord roughness).

### 7\. Timbre Descriptors (MFCC, Transient Sharpness, Modulation)

These are Tier-B (nice-to-have) features per PRD, meaning we should consider them if they’re easy and useful, but they aren’t critical for v1. We list options:

* **MFCC (Mel-Frequency Cepstral Coefficients):** These are a standard set of features that capture the broad shape of the spectral envelope (essentially timbre). We can compute say 13 MFCCs (excluding the 0th if we treat energy separately). The PRD suggests computing mean (μ) and std (σ) of MFCCs over the bar. That yields 26 numbers, which is a bit heavy to include fully in JSON every bar. Instead, we might do something simpler like a *timbre class label* or cluster. But for now, we could compute MFCC and possibly use it internally for patch compatibility scoring (e.g., to measure similarity of timbre between tracks).

**Open-source:** Librosa has librosa.feature.mfcc which needs the Mel spectrogram (which it can compute from the STFT or via its own). That’s fine. It’s a bit of matrix multiply and log – should be okay in 25ms. Essentia also has MFCC implementation[\[32\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=zero,annotations%20based%20on%20SVM%20classifiers). Librosa’s is permissive, we’ll use that.

MFCCs might help identify if a patch is, say, very bright vs dark (first MFCC correlates with spectral centroid essentially), or if two patches have similar tonal quality. Perhaps we can derive a rough classification (like cluster MFCCs to categories: “airy pad”, “sharp lead”, “muffled bass” etc.). That’s not trivial without training, but we could at least detect extremes (if MFCC1 is high it’s bright, etc.).

We’ll plan to compute them but not necessarily output all. We might include a reduced form like **Spectral shape class** if needed. Probably skip from JSON to keep size lean, unless we find a direct use.

* **Transient sharpness:** This can be interpreted as how quickly attacks in the sound reach peak (log attack time) and how much high-frequency content in transients (spectral flux). If our onset detection is already done, we could measure for each onset:

* Attack time: e.g., measure the rise time of the amplitude envelope for that onset. But in a mix, overlapping sounds make it messy.

* Spectral flux: we already get an onset strength envelope from spectral flux. The peak values of that might indicate how sharp transients are. Or zero-crossing rate (ZCR) can increase during percussive transients[\[33\]](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html#:~:text=Applications%20of%20Zero%20Crossing%20Rate%3A) (though ZCR more correlates with high freq content and noise).

Another angle: measure **crest factor** of the waveform or of sub-bands (peak/RMS ratio). A high crest factor in drums means very punchy (transient spikes). If heavy compression or dense layering is present, crest factor goes down (things are smoother/“smeared”).

Simpler: we can output **RMS vs peak** for the bar (peak-to-average ratio). But on a mix, that might reflect overall compression more than just transients.

We might skip explicit transient metric in JSON but use some internally. The “onset\_var” we did is more about timing, not amplitude. Maybe adding **attack\_crest**: e.g., the maximum short-term peak amplitude vs integrated loudness. Not sure if needed in v1.

* **Modulation depth:** The idea is to detect if any LFO-like modulation is present (volume tremolo, filter wah, etc.). Signs of that in audio: periodic oscillation in amplitude or spectrum at a rate \~ a few Hz to a few tens of Hz. We could attempt:

* Take the loudness or a band energy envelope and do an FFT on it to see if there’s a strong frequency component (peak) in the 1–10 Hz range. If yes, that indicates an LFO. For example, a pad with a 5 Hz tremolo would show a peak at 5 Hz in the amplitude modulation spectrum.

* Or auto-correlation of the envelope: a strong repetitive pattern indicates modulation.

This is a bit advanced for real-time, but computing an FFT of a 2-second envelope (which is maybe 1000 samples if sampled at 500 Hz envelope) is trivial. If an LFO is present, we could output its frequency or depth. The PRD suggests “modulation depth indicators” qualitatively. Perhaps just output a boolean or a level (0-1) if we detect noticeable periodic modulation.

We might do this in v1.1, unless time permits a quick hack: e.g., compute the energy envelope of the whole mix in a mid-frequency band (or a specific instrument’s likely band), detrend it, and check if its standard deviation is significant and if its frequency spectrum has a peak. This can produce lots of false signals though (drum loops cause periodic amplitude changes too – but that’s “rhythm” not LFO, though to an algorithm it’s similar: drums at 2 Hz tempo could look like modulation).

Possibly skip for now or mark as future.

**Summary for Tier-B:** We will likely **compute MFCCs internally** to use in scoring (especially PatchCompatibility – see scoring section) but not output all coefficients in JSON to save space. If needed, we can output an identifier like “timbre\_cluster”: e.g., classify each track’s MFCC against known categories (we could do a quick k-means on MFCC of known patch types offline). But that’s sophisticated – perhaps not v1.0.

Transient sharpness and modulation might be omitted in initial JSON, or we include a simple metric if trivial (like global\_attack\_ms average log attack time of onsets, and mod\_depth some normalized variance of 1-10Hz energy).

We’ll clearly separate must-haves vs extras in the implementation plan. The must-have Tier-A (spectral, loudness, key, overlap, width, roughness, groove) are prioritized. Tier-B can be added as we validate performance headroom.

## Scoring Heads (Higher-Level Evaluations)

With the raw features computed each bar, the Listener then computes **score outputs** that represent judgments: how well does the music align with the target genre, how compatible each patch is in the mix, how clear the arrangement is, and safety checks. These scores are on \[0,1\] where 1 \= “excellent” as per PRD, and they will feed into making recommendations.

We will design these scoring algorithms using a combination of **rule-based heuristics** (seeded by domain knowledge) and simple machine learning models where appropriate. Since we likely won’t have a large dataset on day 1 to train elaborate models, we’ll start with interpretable rules (which can later be tuned or replaced by ML with feedback).

Each “head” will output one or multiple numbers per bar (and we might aggregate over 4 bars too). Let’s go through each:

### GenreFit Score

**Purpose:** Rate how well the current music fits the *target genre*. In PRD example, they had "genre":{"house\_fit":0.73, "hiphop\_fit":0.31, "ambient\_fit":0.28} for a target presumably House (since house\_fit is highest). Perhaps they output several genre likelihoods for context, but likely the target is known (“genre\_target”: "house" in context input), so we really only need to output a score for the target genre (plus maybe a couple of other genres for comparison or if we want to show that it partially leans into another genre).

To compute genre fit, we will use a weighted combination of features that characterize genre. From PRD: *“House likes 54–58% swing, steady low, controlled hi air, moderate width…”* They suggest a logistic sigma: σ(w · \[features\]) where features might include spectral centroid, high-band energy, swing, width, roughness, key stability, MFCC projections, etc. In other words, for each genre we could have a linear model or simple classifier.

**Option 1: Rule-based scoring:** We encode some known genre signatures: \- **House:** 4-on-the-floor kick (so steady low energy), usually a bit of swing (around 55%), significant high-hat “air” but not harsh (so some hi but controlled roughness), stereo width not extreme (club tracks often fairly wide synths but centered kick/bass, so moderate overall width \~0.4-0.6). Also tempo range (house \~120-130 BPM, we have tempo from transport). We can include tempo as a check: if user said genre=house but tempo is 90, maybe it’s more hip-hop speed – agent might then question the genre target or suggest a tempo change. \- **Boom-bap Hip-Hop:** typically around 85-95 BPM, strong swing (or human feel, not quantized, onset\_var likely higher), very dominant low mids (kick and snare punch, but not much high fizz), often narrower stereo (older tracks in mono or lightly panned samples), and often a bit of grit (some vinyl noise or saturation – higher roughness maybe). \- **Ambient:** slower tempos or floaty rhythms, often no drums (so low energy might be minimal or at least not “pulsing”), extremely high width (lots of stereo reverbs/pads), high-band presence can vary but generally more smooth (so low roughness, high-band maybe present for airy pads but not harsh), onset variance might be irrelevant (no strong onsets at all in many cases).

We can start by constructing a **score formula** for each target genre as a weighted sum of deviations of features from the “ideal” for that genre, then squashing to 0-1. For v1, a simple approach: \- Define a “target range” or ideal value for key features per genre (we’ll codify what PRD hints and other music knowledge say). \- Compute a penalty for each feature if it’s far from ideal range. \- Sum penalties \-\> invert to a score.

For example, HouseFit might take: \- Tempo: best \~120; if tempo is far (say 150 or 80), score drops. (We know tempo exactly, so easy check). \- Swing: ideal \~0.55; if swing% is 50 (straight) or 60+, slight penalty. House can be straight too, but many house have a bit of swing/groove in hats. \- Low-band energy: house demands a consistent kick/bass, so low should be significant (maybe 0.2-0.5 fraction) and steady. If our low fraction is too low (\<0.15) it’s lacking bass, penalty. If too high (\>0.6) maybe it’s too bass-heavy (uncommon). \- Hi-band energy: House usually has open hats and noise sweeps, so some hi content is expected (maybe 0.1-0.3 fraction). If hi is near zero, track is dull; if hi is extremely high, track might be too bright (some subgenres like tech house are bright though). \- Roughness: House tends to cleaner sound (especially modern house) – so if roughness global is too high (e.g., 0.5 like a very distorted mix), penalty. \- Width: moderate; if width is 0 (mono) penalty (too narrow), if width is \~1 (super wide everything), maybe slight penalty because club mixes keep some focus. But maybe wide is fine too. We’ll set an optimal \~0.5. \- Groove: House often quantized but can have swing. Onset\_var (human looseness) should not be too high – a very sloppy timing might lean more to funk/jazz than house. So if onset\_var \> say 20ms, penalty. If swing is within a band around 55, bonus (if desired). \- Key: Not genre-specific, except some genres (ambient) might be more modal. HouseFit might not depend on key at all except maybe expecting a stable key (lack of key stability might indicate cacophony rarely in house). \- MFCC: If we had genre-specific MFCC patterns, we could incorporate that. For instance, one could train a classifier on MFCC means for genres (classic approach in MIR). However, without training data, we might skip direct MFCC use except perhaps one or two dimensions: e.g., MFCC0 (energy) or MFCC1 (brightness) which overlaps with centroid anyway.

Given time, we will likely implement this as a manual function rather than a learned model, but design it such that later we can replace the logic with a logistic regression or small neural net: \- Possibly gather a small sample of loops or tracks per genre (maybe from user’s own preferences or open dataset like GTZAN) and see feature patterns, then set weights.

**Option 2: Shallow ML model:** If we did want to train a classifier, we could use a small dataset (not sure we have). A logistic regression or decision tree on our computed features to output a probability of genre. But lacking data, we use rules as initial “model”.

**Output:** We will output e.g. genre\_fit for the target genre. The example shows multiple genres, but that might have been for debugging. In production, we might include only the target’s score, unless we want to show the top 2 genre matches (could be interesting for the agent to say “this sounds more like X genre than the target”).

Perhaps better: output a dictionary of several genres’ fits (house/hiphop/ambient as in example), so the agent can see relative scores – that could help if it’s misclassified or cross-genre. Given context includes genre\_target, the agent knows which one it cares about, but the others could be used to suggest “Maybe you intended a different genre?” if target fit is low but another is high. This is speculative but could be nice.

We’ll at least compute for a small set of genres relevant to OP-XY usage (the PRD mentions house, boom-bap, ambient explicitly; we might include maybe “Dnb” or “Techno” later). For now, those three might suffice.

**Performance:** The genre fit computation is trivial math once features are there.

**Academic note:** Genre classification has been studied extensively; state-of-art uses deep learning on spectrograms which can achieve \>90% on datasets like GTZAN[\[34\]](https://cs229.stanford.edu/proj2019spr/report/3.pdf#:~:text=are%20for%20the%20well%20known,networks%20for%20unsupervised%20audio%20classification). But those require big models (CNNs, RNNs). We can’t run those in real-time on CPU (and our agent context doesn’t allow heavy model). But fortunately, a coarse rule-based approach on obvious features might be enough for broad distinctions (house vs hip-hop vs ambient are quite distinct in tempo and spectral shape). There is a risk for more subtle subgenres (Deep House vs Tech House differences are subtle, but PRD suggests possibly asking user “House vs Deep House?” as a prompt if unclear). We can design the system to ask for clarification if genre fit is middling or ambiguous between two (e.g., track has elements of two genres).

For now, we implement with domain heuristics and improve with user feedback (like PRD v1.1 mentions preference learning via A/B thumbs which could eventually adjust these weights or train a model).

### Patch Compatibility Score (per Track)

**Purpose:** Score each patch/track on how well it **complements the other sounds** in the mix. If a particular instrument is out of place (too loud, conflicting frequencies, off-key, etc.), it should get a lower score so the agent knows that’s a swap candidate.

**Approach:** This is effectively evaluating the **mix interplay** from the perspective of each track. A track’s compatibility could be reduced by: \- Frequency masking with another track (we computed overlaps). \- Key or harmonic conflict (if one track plays notes not in the key or clashes with another’s harmony – e.g., if one is in a different scale or tuning; our key detection might catch if multiple tonal centers). \- Stereo conflicts (if two big wide stereo sounds sum to an unfocused image – but this is hard to quantify, perhaps if two tracks both have high width and overlap in freq, we might penalize). \- Redundancy in timbre or register: e.g., two instruments filling the same role. If the lead synth and pad have very similar timbre (say both saw waves in same octave), they might mask each other and also be redundant – one should change patch or be EQ’d. \- Role typicality: If a patch doesn’t fit its role expectations (e.g., a “bass” track that actually has no sub-bass, or a “lead” that’s too soft attack to lead a melody), that patch might not be ideal. We can use MFCC or spectral shape vs known role patterns. For instance, a bass patch should have strong low-frequency content (if not, patch\_fit for bass goes down). \- Relative balance: If one patch’s volume is much louder/softer than others (though loudness should be handled at mixing stage by agent, but patch might inherently be too loud or quiet dynamic). If one track’s LUFS (we could measure track-level loudness if we had stems or by isolating via times it plays alone) is off, could mark it. This might be beyond v1 to measure precisely.

**Implementation idea:** We can aggregate various metrics into a patch score for each track: \- Start from 1.0. \- Subtract some amount for each significant conflict involving that track: \- If kick\_bass\_overlap is high, subtract from both kick and bass compatibility. Possibly weigh more on the one that “should change”. Typically, if kick and bass conflict, which to swap? Likely the patch that’s less genre-appropriate or more easily changed. Kick could be a sample – swapping kick or sidechaining could fix. Bass patch might be causing conflict if it’s too subby or has too long release. Perhaps agent would try EQ or sidechain first. This is tricky: we may just alert both or decide on one. The PRD example recommended swapping the lead patch to fix lead-pad overlap. It could as well have suggested changing pad. Possibly they assume pad is more fundamental and lead can be tweaked. Or maybe lead had a lower compat score. Actually their patch\_fit: lead 0.74 vs pad 0.69 – pad was lower, yet they suggested swapping lead. Perhaps because swapping pad would alter harmony more? Or because lead swap might yield bigger genre fit improvement.

We might in recommendations prefer swapping melodic patches over harmonic background for a genre change. But for scoring, let’s just score both down for conflict.

* If a track is out-of-key: if key detection finds the whole mix key conf is low, maybe one track’s notes are causing that. We could attempt per-track key estimation by isolating that track (not feasible via audio easily). But if we trust the loop JSON, if the user gave a key for the loop and a track uses many notes outside that scale, that track might be causing dissonance. The Validator might normally catch scale degrees if specified. But if user doesn’t specify scale and just gave notes, identifying culprit of dissonance is complex. Possibly skip for now, assume user composing through agent would align key mostly. If key\_conf is low, maybe penalize all melodic tracks a bit.

* If a track’s **spectral footprint overlaps heavily** with another, we penalize both slightly, or one more if it’s stepping on the other’s territory. For example:

* Lead vs Pad overlap high: If pad is meant to support, maybe the lead patch should cut through more (so blame lead being not cutting enough, or pad being too in same range). Could go either way. Possibly base on roles: A lead is expected to be prominent, a pad should sit under. If overlap is high, maybe pad sound is too bright/loud (should be more background). So *maybe penalize the pad more* in that scenario (since pad compatibility would increase if it didn’t clash). However, the example had pad fit lower, which it was (0.69 vs lead 0.74) – consistent with pad being penalized a bit more for overlap. Yet they chose to suggest swapping lead. Perhaps because pad might be fundamental to harmony while lead could be a more experimental sound to replace. This is subtle. For automated scoring, we might just penalize both somewhat and let the agent decide which to actually change based on context or ease.

We can refine with domain logic: e.g., if lead\_pad\_overlap and **lead patch genre fit is also low**, then swapping lead yields two improvements (genre\_fit↑ and less overlap) – exactly what their recommendation said (“reason: genre\_fit ↑ \+ mask ↓”). So the system might choose the track that when swapped can fix multiple issues. So we will provide both sets of scores and let the agent logic decide which action yields best multi-metric gain.

* If a track **role is drums** (kick, snare, hats) we might handle differently: e.g., if hats overlap snare, maybe we penalize hats patch (use a shorter hat or none on snare hits). If kick and bass conflict, often solution is sidechain or change bass patch envelope – either way, note it.

* **MFCC similarity approach:** We could measure pairwise cosine similarity between tracks’ MFCC means. If two tracks have extremely similar timbre (e.g., two synths with saw wave), they might mask each other or sound redundant. Penalize both a bit for being too similar. Alternatively, penalize the one that is *musically less needed*. Hard to automate who is less needed. Could ignore this for now or implement lightly.

* **Spectral holes:** If a track covers a unique frequency range that others don’t, that’s good (complementary). If all tracks overlap a lot in spectrum, not good. We can measure how much each track *contributes unique spectrum*. Without source separation, one trick: compare the mix spectrum to the mix spectrum with that track “subtracted”. We can’t do that precisely without audio stems. But with loop info, we might approximate: e.g., if a track is high-pitched (like a hi-hat), it obviously contributes high frequencies which others might not. We can infer from role: hats contribute highs that others might not, so hats patch likely fine (unless harsh). Bass contributes sub. Pad and lead might overlap in mid. So these reasoning are already in overlaps.

Given limitations, we will implement a **simple aggregate**: For each track: \- Initialize score \= 1.0 \- If track role is melodic (bass, lead, pad) and overall key confidence is low, reduce score maybe 0.1 because it might be contributing dissonance. \- Subtract half of each overlap metric that involves this track. E.g., if lead\_pad\_overlap=0.5, subtract 0.25 from lead and 0.25 from pad. If kick\_bass\_overlap=0.2, subtract 0.1 from kick and bass. \- If track’s spectral energy is very unbalanced relative to expectations for its role: \- e.g., Bass track but low-band energy of mix is low \-\> maybe bass patch not actually bassy \-\> low patch\_fit. \- We can gauge this by: if this is “bass” role track, and low band fraction \< some threshold (or if we had track isolated loudness in lows from pattern – not directly but if bass plays notes high up or has a high-pass filter, the mix low might be less). Possibly the agent wouldn’t label something bass if it’s not bassy, but who knows. We might skip or leave to human/LLM. \- If track has an extreme width that is problematic (e.g., a bass patch with too high width can cause phase issues, typically bass should be more mono): if role is bass and global stereo width is high entirely due to bass (hard to tell from mix though – but if bass is stereo patch, it might spread lows which is undesired). Possibly we enforce: if role=bass and width \>0.7, penalize bass patch a bit (because maybe better to use a mono bass patch or sum lows mono). Similarly, if role=kicks (drums) and width \>0 (kick should be dead center ideally), but again from mix we can’t isolate kick width directly, presumably kick is mostly mid-channel anyway or sample.

This might be too much guess; perhaps skip width by role for now except note if something obvious.

* Use MFCC to identify outlier: If one track’s MFCC (timbre) is very different from others and also maybe far from genre typical, it could be either very good (it stands out intentionally) or bad (it doesn’t blend). Hard to quantify automatically without training.

Given these complexities, initial patch\_fit will lean heavily on the **overlap and mix balance factors** we can derive. That should already pinpoint obvious conflicts (e.g., pad and lead fighting, bass and kick fighting). The fine-tuning (e.g., “patch doesn’t fulfill role timbre”) might come from human rules: For example: \- If genre=ambient and track role=drums with loud transients, that patch has low fit (drums might be too aggressive for ambient). \- If genre=hiphop and role=hats but hats patch is extremely bright white noise, maybe that’s fine for some, but if it was ambient, that’d be terrible.

Actually, this strays into *genre-fit per patch* territory. We could break PatchCompatibility into: \- *Patch genre fit* (is this patch sound appropriate for genre in isolation? e.g., an 808 kick vs an acoustic kick – one fits hiphop, one maybe not as much for house). \- *Mix compatibility* (masking, etc.).

The JSON example compat.patch\_fit presumably combines those (the values might reflect both internal mix and genre appropriateness). They recommended swapping a lead patch “for genre\_fit ↑ \+ mask ↓” – implying the current lead patch wasn’t ideal for genre either. So indeed, patch\_fit likely factors in both genre-specific timbre and interactions.

**To incorporate patch genre-fit:** We might leverage a small knowledge base of patch types: If we know the preset names (e.g., "prism\_ld\_03", "wt\_bell\_02" as in recommendation candidates), maybe engine “Prism lead 03” vs “Wavetable bell 02” – perhaps one is more suitable. We don’t have that knowledge right now. In future, we could classify each preset by brightness, warmth, etc., or even tag them by genre manually. For now, lacking that, we rely on audio features: \- e.g., lead patch in house: ideally a bright, plucky or sustaining synth. If our lead patch is currently dull or too percussive, its spectral centroid or envelope might show that. We can guess: if lead track’s high-frequency content is low, maybe it’s not cutting enough for house (house leads often have some brightness). Hard without isolation though.

Given time constraints, we’ll implement a **basic compatibility score formula** and mark places for improvement. The output goes in the JSON under compat.patch\_fit, mapping track IDs to a 0-1.

We will ensure to keep it *relative* more than absolute: i.e., if two tracks are the main conflict, their scores might drop into e.g. 0.6-0.7 range while others remain 0.8-0.9. That way the agent sees which 1-2 are lowest and targets them. (In the example, bass was 0.61, pad 0.69, lead 0.74 – bass lowest, pad next. But they swapped lead – interesting, maybe because changing bass might alter groove too much? Or they had reason beyond just numeric lowest. Possibly because improving lead could give more genre boost if bass maybe was needed for low end. That might be something encoded in those recommendation heuristics.)

Anyway, we will output these and possibly provide reasoning (in recommendation step).

### Arrangement Clarity Score

**Purpose:** Evaluate the mix clarity from an arrangement perspective – does each instrument have its own space in the frequency spectrum and musical function, or is everything clumped and muddy? Also, is there a stable musical focus (tonal center)?

From PRD: *Penalize when 3+ roles cluster in same band; reward distributed energy and stable tonal center.* This suggests: \- If low, mid, high are all occupied by at most 1-2 instruments each (distributed), clarity is good. \- If, say, low is just bass, mid is lead, high is hats (well separated), clarity high. \- If low: kick+bass (2 in same band, but 2 might be okay if they alternate – but if both simultaneously heavy, that’s masked – which our mask metrics cover), mid: lead+pad (2 in same band), high: hats (1). So mid has 2 roles overlapping heavily \-\> clarity suffers a bit. \- If everything sits in mid (like guitar, piano, vocal all overlapping mid frequencies) and little bass or treble, arrangement is congested \-\> clarity low.

We can quantify clarity via **entropy or concentration of spectral distribution**: \- Compute what fraction of total energy each track contributes to each band (if we could isolate tracks, easy, but no). \- Or simpler, use the band energy fractions (lo/ mid/ hi) of the *mix*: if one band (mid) has \>\> others, that means most energy is in one band (could indicate clutter in that band). Balanced spectrum might mean parts are well spread. But careful: some genres legitimately have mid-heavy mixes (like a solo piano would concentrate in mid but that’s not “unclear” if it’s just one instrument; clarity issue arises when *multiple* sources in same band interfering).

So perhaps: \- If **band energy entropy** is low (means energy is dominated by one band), then if multiple instruments, clarity suffers. If dominated by one band but also only one instrument exists, that’s not clutter just sparse instrumentation – which might actually be clear since nothing competes. \- So should factor number of instruments active vs band distribution. If 3+ instruments and band entropy low (they all pile in same range), bad. \- Key stability: a stable tonal center (one clear key) is part of clarity. So if key\_conf \< 0.5, clarity score should drop (since that often sounds messy unless intentional polytonality). \- Another measure: **spectral spread** – how wide the frequency content spans. If everything is e.g. 200–1000 Hz and nothing in bass or treble, mix might sound boxed. But that could be stylistic (lo-fi sound). However, by general standard, a full-range mix with some lows and highs tends to be clearer (assuming not all instruments jammed in mid). \- We can incorporate **overlap count**: If more than 2 instruments overlap in a band, penalize. E.g., if we had guitar, piano, vocal all in mid frequencies overlapping, clarity poor. Our pairwise overlaps detect pairs, but if all pairs overlap, that implies 3 in same band. We can attempt to detect triple overlaps by analyzing pair overlaps collectively: \- For each band, see how many roles have significant energy there. Hard without stems, but maybe by role: \- Drums (kick/snare) occupy low and mid, hats high. \- Bass: low (maybe mid some harmonics). \- Pad: mid and maybe some high. \- Lead: mid-high. If we guess from roles: mid-band likely has snare, pad, lead all present \= 3\. High might have lead’s harmonics \+ hats (2). Low has kick+bass (2). Mid having 3 is indeed a cluster. So in that scenario, arrangement clarity down. We could actually use our roles list and assign each role a primary band: \- kick & bass: low, \- snare & chords: mid, \- lead: mid-high, \- hats: high. Then count roles per band. But this doesn’t account for actual patch sound (maybe pad has a lot of high too if bright). But as a rough metric, if any band has ≥3 roles actively contributing, clarity \-.

Given the complexity, we may implement clarity as a **weighted combination** of: \- *Band balance:* use something like 1−Hp where H is entropy of \[lo, mid, hi\] distribution (normalized). A high entropy (balanced spectrum) is good (so maybe clarity score component \= H). If one band dominates (low entropy), clarity suffers. But note: if that one band dominance is because only one instrument exists, maybe not an issue. Perhaps multiply entropy by a factor related to number of tracks or presence of multiple overlapping roles. \- *Key stability:* add a term for key\_conf (0 to 1). \- *Overlap density:* we can sum up all pairwise overlap values. If a lot of overlapping pairs (especially \>2 overlaps), clarity down. For instance, if lead\_pad \+ kick\_bass \+ hats\_snare all significant, that means across the spectrum things are colliding. We already calculate these overlaps; we could define an “overlap index” \= (lead\_pad \+ kick\_bass \+ hats\_snare \+ maybe others) / N\_pairs. But since those are limited, maybe just do sum and cap.

Alternatively, realize arrangement clarity is somewhat the *inverse* of overall masking and chaos. Possibly we can derive it from other metrics: \- Low clarity if: multiple overlaps are high, key\_conf low, roughness high (lots of noise can reduce clarity), band unbalanced (like missing highs or lows which can make mid clutter worse). \- We might incorporate roughness in clarity? Roughness was more timbre, but a very noisy mix can mask detail, reducing clarity perception.

To keep things simpler: **Proposed clarity formula (tentative):**

 \\text{clarity} \= w\_1 \* \\text{entropy}\_{bands} \+ w\_2 \* \\text{key\_conf} \+ w\_3 \* (1 \- \\text{avg\_overlap}) 

with weights adjusting relative importance. Where avg\_overlap is average of our overlap metrics (or a weighted sum if some overlaps matter more). We want clarity high when band entropy high (well spread frequencies), key\_conf high (everything in one key), and overlaps low (instruments not masking much).

We’ll test some dummy values. For example: \- If track has balanced lo/mid/hi (entropy \~1), key\_conf 0.8, overlaps \~0.2 average, clarity could be high \~0.8-0.9. \- If everything mid (entropy low \~0.5), key\_conf 0.4, overlaps 0.5, clarity might drop \~0.4.

We output one clarity score per bar (or per summary?). Possibly just in summary or per bar. The PRD doesn’t explicitly show clarity in the bar JSON example, maybe they consider it a higher-level metric. They listed it in heads but not in the JSON snippet. Perhaps it’s rolled into flags (like they flagged “lead\_pad\_overlap” and “harsh\_hi” as top\_flags, implicitly clarity issues).

We might not output clarity as a separate field every bar to save space, and instead highlight issues via flags and use clarity internally. But a numeric clarity might be useful too for Codex to see progress (like “arrangement\_clarity: 0.85”).

We can include it in summary if not each bar.

### Safety Checks

**Purpose:** Identify any technical audio issues that should be fixed for a good mix: \- Clipping (digital distortion if output too loud). \- Loudness out of reasonable range. \- Too harsh high frequencies that could be fatiguing. \- Possibly DC offset or something (not likely here). \- According to PRD: “clip guards, LUFS corridor (e.g., –14 ±2), harshness caps.”

So: \- **Clipping:** If mix.tp\_max (true peak) \>= 0 dBFS or very close (within, say, 0.1dB), that’s a problem. We should flag it. The Listener can output a boolean or add to top\_flags e.g. "clipping" if detected. Also, it could reduce patch compatibility for whichever track is causing clip (maybe a too loud patch or too much sub-bass). But easier: just flag at mix level and in recommendations suggest lowering something or engaging limiter. \- **LUFS corridor:** If integrated loudness is far from \-14 LUFS (for a final mix, \-14 is often a target for streaming normalization). If the track is too loud (e.g., \-8 LUFS, very compressed) or too quiet (-24 LUFS), flag. But since we can’t adjust overall volume via patch selection (that’s more mixing), maybe it’s just a note. The agent (Codex) might adjust the masterGain or something if we had that parameter. In OP-XY context, maybe user can adjust volume per track. If one track is causing too loud of mix, we might recommend turning it down. However, volume adjustments might be outside PRD scope (the agent might directly do small patch velocity tweaks or tell user to lower output).

* **Harshness cap:** If rough.hi\_band \> threshold (say 0.3) or if high frequencies are dominating (hi energy fraction too high and flatness high), we flag “harsh\_hi” (as in example top\_flags). The recommendation might be to load brighter hats if *lack* of air, or conversely if too harsh, maybe “reduce resonance or EQ high frequencies”.

* **Other safety:** Possibly if there’s sub-bass overload or phase issues (less likely to measure).

We’ll incorporate safety checks as *flags and recommendations* rather than numeric scores. The “Safety” head in PRD might just gate certain values: \- If LUFS not in \[-16, \-12\] range, note it. \- If clipping occurred (peak \> \-0.1), note it. \- If harshness beyond threshold, note it.

Those could be output as boolean flags in notes or in top\_flags.

We can also combine into a single safety score (1 if all good, lower if issues). Not sure if needed. Possibly not, better to explicitly flag.

The agent likely uses these to avoid doing something if unsafe (like if clipping, maybe reduce levels).

We will thus: \- Set mix.clip\_s to maybe number of clipped samples or peak in dB (we have to clarify what PRD intended by clip\_s). The example had 0.0, maybe meaning 0 dBFSpeak (not clipping). If it was clipping, perhaps it would show positive dB value above 0\. \- Provide a top\_flags array in summary listing any safety or major issues (like "lead\_pad\_overlap", "harsh\_hi", "clipping"). The PRD example’s top\_flags included "harsh\_hi" which is a safety/tonal issue.

**Recommendation triggers:** Safety flags will lead to certain recommendations: \- If clipping: recommend lowering a certain track volume or applying limiter, etc. Possibly a device nudge: “Lower master volume or enable limiter (device-only action)”. \- If harsh\_hi: as they did, recommend adding a brighter element to fill good highs if lacking, or if too harsh, maybe “reduce filter cutoff on lead” or “apply EQ dip at 5k” etc. In example, "Load brighter hats" was recommended because *air 8–12k lacking* – interestingly they flagged harsh highs but the recommendation was to add highs (meaning the harshness might have come from mid-high 6k region from lead, but overall not enough 10k sparkle from hats – so fix by adding bright hats to cover 8–12k, thereby making high end smoother perceived). This shows a nuanced approach: sometimes harshness isn’t solved by cutting harsh content, but by filling *pleasant* high frequencies around it to mask it or balance it. That’s advanced mixing logic.

Given complexity, initial rec for harshness likely simpler: if too harsh, maybe suggest using a less harsh patch or a filter. But their approach implies: harsh hi (like a peak at 6k) \+ lacking air \=\> add air (brighter hats) to mask/hide the harshness in context. That’s a creative approach. We might or might not implement that automatically. Possibly the LLM will infer that solution if we just report "harsh hi and lacking air". The recommendation text they gave might have been something the system constructed via rules though.

Anyway, we’ll at least detect these conditions and list in flags/recommendations as appropriate.

## Recommendations Generation (Actions)

Finally, the Listener will output a set of top **recommendations** for improvements, each with a reason and an expected impact (delta in metrics)[\[6\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/docs/prs/0001-initial-docs.md#L6-L9)[\[35\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/docs/prs/0001-initial-docs.md#L26-L34). This is less about numeric analysis and more about decision logic using the scores above.

We have to map metric findings to actionable suggestions: \- **Patch swaps:** If one or two patches have notably lower compatibility or genre fit, suggest swapping them. Our system can even shortlist some candidate patch IDs (as in example) if we have a database. Lacking a database of patches by timbre/genre, we might have to rely on the LLM to know or search. But maybe the system can at least say "swap lead01 with a brighter lead patch". In example they gave actual preset IDs – possibly the agent knows from somewhere or we could maintain a small mapping (like a dictionary of recommended patches per scenario). We might not implement the actual candidate list in v1 (the LLM could generate based on its training if it knows some patch names or using a function to query a patch library by tags). However, since the PRD explicitly shows candidates, maybe they expect us to pre-compute some. Perhaps OP-XY has a finite set of presets; we could categorize them offline. For now, we can include dummy IDs (or simply none, letting Codex fill it). But the UI likely expects some options. We might integrate with an upcoming feature: e.g., if the user has a database of patch metadata, the agent could query something like “find 3 lead patches with higher brightness”.

Anyway, triggers for patch\_swap recommendation: \- A track has low patch\_fit score AND changing it would improve genre\_fit or reduce masking. For example, if lead patch is off-genre and also overlapping pad, swapping it could yield improvement in both “genre” and “mask” metrics. We detect this combo and output rec. \- If bass is too subby and overlapping kick, might suggest a different bass patch with less sub or shorter tail. \- These suggestions will mention the reason: e.g. "genre\_fit ↑ \+ mask ↓" meaning it will raise genre alignment and reduce masking overlap.

* **Engine param nudge:** If a patch is close to good but just needs minor tweak (like “open the filter to add brightness” or “increase envelope decay to shorten notes”), we could recommend a parameter change. We know OP-XY engines have fixed CC mappings (docs mention a CC map). For instance, “Axis P1 \+8 (open LPF)” as PRD suggests. We can identify such nudges if metrics show a slight shortfall in one area:

* E.g., high-band lacking air but otherwise fine: recommend increasing filter cutoff on a synth or adding a bit of high EQ. The PRD example in recs: "device\_nudge": {"title":"Load brighter hats", "why":"air 8–12k lacking"} – that was labeled as device-only because loading a new sample is not something the agent can do via JSON (the user must do on the OP-XY device). So they presented it as a suggestion for the human. We might output these as type:"device\_nudge" with a title and reason.

* Another nudge: “Axis P1 \+8” meaning if track uses Axis engine, param1 is likely filter, so they suggested adjusting it. That could actually be done via JSON CC automation (maybe the agent *can* do that by adding a CC lane in loop JSON), unless they consider it requiring manual user action. The PRD said “Engine param nudge e.g. Axis P1 \+8… the agent can act on that since it's JSON edit (it can insert that CC value or preset parameter change).

* So we might have two subtypes: **patch\_swap** and **param\_nudge** that agent can do, and **device\_nudge** for things only the user can do (like change an OP-XY hardware setting or load a sample).

* **Arrangement hints:** e.g., “Thin pad width to 0.25; widen lead to 0.5” as PRD suggests for overlap. If we detect lead\_pad\_overlap, another fix aside from patch swap is panning or narrowing one instrument’s stereo width. If our system allowed controlling patch width, perhaps via CC or selecting mono mode, we could suggest it. Maybe OP-XY doesn’t have direct width control except via effects. But at least telling the user “make pad narrower” might be possible. However, since we can’t enforce that easily in JSON, it’s more of a human suggestion. Possibly a device\_nudge again (“Pan pad to side” or such).

* Or “reduce arrangement density: mute one instrument here” – but that’s more creative, probably not in v1 auto suggestions.

* **Device-only tasks (nudge cards):** These are things the user must do on the OP-XY physically or in UI, which the agent cannot. The PRD gave examples like “load brighter hats” (meaning pick a different sample on device) or “swap FX chain” or “record a texture sample” or “map tilt to cutoff” – these are beyond JSON editing (more about hardware or performance). We can output these under type device\_nudge with a short title and why. They should be limited to high value suggestions. We’ll likely only include if something is clearly lacking that requires user intervention (like needing an entirely new instrument track or an FX that the agent cannot create because JSON doesn’t cover adding devices).

Given our analysis outputs, our recommendation logic will likely be: \- Look at **genre\_fit**: if below some threshold (say \<0.7), find which track’s patch is most responsible (lowest patch\_fit or lacking typical genre character). Recommend a patch swap for that track to something known to be more genre-appropriate. (We may have to hardcode some known patch differences or rely on descriptive names; e.g., if current bass preset name contains “sub” but in hiphop maybe a gritty bass is better, etc. Might lean on LLM’s knowledge base if connected). \- Look at **compatibility**: if a track has very low compat (and not just due to genre), recommend swap if not done already. \- If overlaps are moderate but not severe, maybe a simpler fix: e.g., if lead\_pad\_overlap moderate and both patches otherwise fine, suggest width/pan adjustments or EQ. But since we can’t do detailed EQ, maybe suggest “reduce pad synth cutoff to leave room for lead” or “sidechain bass to kick” if kick\_bass\_overlap (sidechain might be a device nudge card: user sets up sidechain compression). \- If **safety issues**: \- Clipping: recommend “lower master volume or track X volume” (if we can identify which track peaks – we could approximate if one track is likely culprit e.g., the bass). \- Loudness too low: recommend raising something or just note it (maybe user can normalize at mastering). \- Harshness: if harsh\_hi flagged, and if there is any easy fix like if lead is very resonant – suggest using a mellower patch or lowering that resonance (if filter Q is a param, or “use a lowpass EQ on lead”). \- Lack of highs (dull mix): suggest adding an element or adjusting patch to add highs. They chose “add brighter hats” which implies maybe the current hat sample was dark or no hats at all. If our analysis shows hi fraction very low, that might be the call.

We will produce at most 2-3 recommendations (to not overwhelm the user). The PRD explicitly says top K actions with confidence and short why. We’ll output them as an array in summary JSON. Each rec has: \- type: "patch\_swap" | "param\_nudge" | "device\_nudge" | etc. \- details like "track": "lead01" if applicable, and perhaps "candidates": \[...\] for patch swaps (if we have any). \- reason or why: a short explanation string (\<=90 chars). E.g., "genre\_fit↑ \+ mask↓" or "air 8–12k lacking". \- possibly a confidence score (like 0.79 in PRD example). We can calculate a simple confidence \= some function of how much metrics improve if this action is taken (we can estimate metric deltas qualitatively or just output a heuristic number). For now, maybe confidence can correlate to how bad the current situation is (if patch\_fit is very low and known fix likely helps, confidence high).

At first, we can output without heavy computation of deltas (just put an expected improvement in delta fields like PRD did for house\_fit and overlap). For example, if we swap lead, we expect house\_fit to maybe increase by X and overlap to decrease by Y. We can guess X and Y from differences to next best patch if we had that info. Without actual audition, we might not truly know those deltas. But PRD’s example suggests we should provide them. Possibly the LLM agent itself can simulate audition or just trust the suggestion; providing deltas maybe is just to help LLM reasoning.

We could do a quick hypothetical: if lead patch changes to something 10% brighter, maybe house\_fit \+0.06 and overlap \-0.12 as in example. These might be rough numbers. We might create these deltas by: \- Taking difference between current metrics and an ideal scenario for that track. E.g., if lead patch\_fit is 0.74 and we think a perfect lead could be 0.85, we put \+0.11. But that’s speculation. \- We could just give small deltas like \+0.05 to \+0.1 to indicate improvement.

Given time, maybe skip actual numeric delta calculation; but PRD shows it, so maybe do minimal: \+0.05 increments for the primary metrics it affects.

**Human-in-loop prompts:** Not exactly recommendations, but the PRD section 9 describes questions to ask the user (taste preference clarifications). We may implement logic that if the analysis can’t decide something (e.g., “airy vs warm” pad – if both could work and agent needs user’s taste), it would queue a question. Possibly the LLM will handle asking, but the Listener could signal need for it. Perhaps via a special recommendation type or a note like {"type":"query\_user", "question":"Pick 2: airy or warm?"}. This is more complex because it involves conversation, which might be solely the LLM’s territory. The Listener might just detect “lack of clarity on brightness preference” if metrics in a grey zone. We may leave this to the agent logic to generate based on metrics.

For now, focus on concrete recommendations.

**Licensing of knowledge:** Our recommendation logic itself doesn’t require external code, just our rules and data. If we had a knowledge base of patches or learned parameters, we must ensure it’s from allowed sources (likely fine if using internal data).

---

## Feasibility and Open-Source Considerations

**Performance Feasibility:** All the described feature computations are lightweight DSP operations that can be done well within 200 ms on an M1: \- Audio capture via CoreAudio is real-time capable (we just need to buffer \~2s). \- Loudness (pyloudnorm) might be the slowest single calculation due to gating algorithm, but for 2s of audio it’s negligible (pyloudnorm’s own paper shows it’s quite efficient[\[14\]](https://www.eecs.qmul.ac.uk/~josh/documents/2021/21076.pdf#:~:text=%5BPDF%5D%20Convention%20Paper%2010483%20,is%20both%20fully%20compliant)). \- Spectral features via numpy FFT are extremely fast on Apple’s Accelerate. Librosa’s heavy parts (FFT, CQT) use numpy (Fortran or FFTPACK under hood, vectorized). \- Key detection (chroma \+ correlation) is trivial overhead. \- All feature algorithms are either linear in signal length or frame count – we’re dealing with maybe \~100 frames per bar, so very small. \- The scoring and recommendation logic is just a few arithmetic ops and conditionals.

Memory-wise, we store at most a few seconds of audio and some spectra. \<250MB is easily met (our arrays will be on the order of a few hundred KB per bar).

**Open-source libraries:** We plan to use **librosa** (ISC license, similar to BSD) for many features (spectral centroid, chroma, MFCC, onset strength, etc.) – it’s well-maintained and lightweight. We’ll use **pyloudnorm** (MIT) for loudness[\[19\]](https://github.com/csteinmetz1/pyloudnorm#:~:text=License). For audio I/O, **sounddevice** (MIT)[\[8\]](https://github.com/spatialaudio/python-sounddevice#:~:text=spatialaudio%2Fpython,see) or **soundcard** (BSD)[\[9\]](https://soundcard.readthedocs.io/en/latest/#:~:text=SoundCard%20is%20licensed%20under%20the,clause%20license) are both acceptable. No licensing conflicts with our code (we can include them, they’re permissive).

We will **avoid GPL/AGPL libraries** like aubio (GPL3)[\[29\]](https://github.com/aubio/aubio#:~:text=aubio%20is%20free%20software%3A%20you,the%20Free%20Software%20Foundation%2C) and Essentia (AGPL3)[\[36\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=because%20these%20techniques%20allow%20novel,be%20complemented%20with%20Gaia%2C%20a) in our runtime due to licensing, unless we decide to open source the entire project under a compatible license. If at some point Essentia’s advanced algorithms are highly desired (key detection accuracy, dissonance calc, etc.), we could either: \- Negotiate a commercial license for Essentia (they offer that for closed-source usage[\[27\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=analysis%20and%20audio,classifying%20the%20results%20of%20audio)). But that might not be needed if our simpler methods suffice. \- Use Essentia in a separate process or service (to isolate GPL code). Probably overkill.

**Latest research integration:** We have drawn on MIR research for feature definitions (HPCP for key, spectral descriptors for timbre, etc.) and the system is designed to allow improvement. For example, in the future we could incorporate a tiny **neural network** to refine genre detection or patch recommendations: \- Perhaps use a pre-trained music embedding model (like an open model that outputs a vector for genre/mood) and do a quick inference. There are lightweight ones like *musicnn* (if we prune it) or *YAMNet* (but YAMNet is more for sound events, not music genres). Running a small conv net on a 2s spectrogram could be done on the M1 neural engine or GPU with CoreML, possibly within 50ms. But that adds complexity and dependency. For now, rules will do.

* For patch compatibility, a future approach could be a learned model that given all feature metrics decides a score (like a small random forest or NN). We could even gather training data by simulating some loops and having human label best patch combos. This is beyond v1 though.

The design as specified is **feasible with open-source tools** – nothing is fundamentally unsolved. The key is tuning the heuristics to get meaningful results. We will likely iteratively refine thresholds by testing on known examples (the “Validation & QA plan” in PRD suggests golden loops per genre, and patch A/B tests to ensure the Listener agrees with human rating ≥70% of time).

**Codebase fit:** We will implement this Listener in Python, which aligns with our Conductor and Agent (which are also in Python, per earlier design docs). We’ll integrate it either as part of the Conductor process or a sibling process. The functions outlined in PRD (start\_listener, analyze\_bars, etc.) will be implemented perhaps in a module like listener/: \- listener.capture using sounddevice/soundcard \- listener.analyze computing features (likely using librosa or numpy) \- Possibly a small listener.model that houses the scoring rules (so we can adjust easily or swap out with ML later). \- We’ll also include a JSON Schema for BarPacket and SummaryPacket in docs/ as offered.

We should ensure the output JSON formatting (short keys, etc.) is consistent with the opxyloop style (though opxyloop JSON is separate, but consistency in naming and style helps). The PRD uses concise keys (lo, mid, hi, etc.), which we will follow.

We also must ensure **thread safety** if running in same process: capturing audio while the Conductor is doing file I/O and WS – Python GIL means one thread at a time in pure Python, but if sounddevice uses C callback, that runs in C thread and fills buffer independent. We might need to use a separate process or the sounddevice callback should be minimal (just buffering then we analyze after copying buffer).

We will test on Apple M1 with the Scarlett to measure actual CPU usage. If by any chance Python is too slow (which we doubt for these sizes), we could consider using **NumPy on multiple threads** (maybe MKL multi-thread FFT? But Accelerate likely single-thread for small FFTs, which is fine). If needed, we could offload some heavy ops to a C++ extension in future (like we could compile Essentia’s relevant algorithms into a dynamic lib and call via CFFI for speed if absolutely needed).

However, given our scope and test loops, this should be fine.

**Conclusion:** It is absolutely feasible with current open-source tech to build this Listener/Evaluator. We will proceed with the above choices (librosa, pyloudnorm, sounddevice), keeping in mind license compatibility (all MIT/BSD) and leaving the door open for ML upgrades (which might use something like PyTorch – but that’s a heavier dependency we might avoid unless needed for accuracy in later versions).

In the next pass, we’ll firm up these choices into a concrete design, specify exact thresholds/weights, and detail the data structures and control flow, ensuring the implementation meets the product requirements and integrates smoothly with the existing OP-XY vibe coding environment. For now, we have laid out the available options and the reasoning behind preferred solutions, showing that the Listener can be built with modern MIR techniques and open-source tools, achieving the desired real-time, bar-synchronous insight for the music agent.

**Sources:**

* Essentia features (AGPL, extensive audio descriptors)[\[23\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=loudness%20,key%20and%20scale%2C%20tuning%20frequency)[\[32\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=zero,annotations%20based%20on%20SVM%20classifiers)

* Librosa feature extraction (spectral centroid as “brightness”)[\[20\]](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html#:~:text=Spectral%20Centroid)[\[21\]](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html#:~:text=,the%20%E2%80%9Cbrightness%E2%80%9D%20of%20a%20sound)

* SoundCard audio I/O (CoreAudio, BSD license) example usage[\[31\]](https://soundcard.readthedocs.io/en/latest/#:~:text=,get_microphone%28%27FS2i2)[\[11\]](https://soundcard.readthedocs.io/en/latest/#:~:text=,abs%28data%29%29%2C%20samplerate%3D48000)

* pyloudnorm loudness (ITU BS.1770-4, MIT license) usage and compliance[\[16\]](https://github.com/csteinmetz1/pyloudnorm#:~:text=Implementation%20of%20ITU,weighting%20filters%20for%20additional%20control)[\[17\]](https://github.com/csteinmetz1/pyloudnorm#:~:text=,integrated_loudness%28data)

* Genre classification approaches (traditional vs deep) noting MFCC effectiveness in short windows[\[37\]](https://cs229.stanford.edu/proj2019spr/report/3.pdf#:~:text=genre%20with%2064,business%20value%2C%20but%20of%20the)

* Frequency masking concept (overlap in same range causes clarity issues)[\[30\]](https://www.izotope.com/en/learn/what-is-frequency-masking?srsltid=AfmBOoqF27633mIFtuAuALAhpmzAgtqwIB5yZVXiIwNIN2FU4PDGW0cl#:~:text=What%20Is%20Frequency%20Masking%3F%20,in%20the%20same%20general%20location)

* Hermes 2019 on spectral clarity predictors (harmonic centroid & mid-band peaks)[\[24\]](https://www.arpjournal.com/asarpwp/wp-content/uploads/2021/12/Kirsten-Hermes_ARP2019.pdf#:~:text=As%20established%20in%20a%20prior,are%20established%20in%20this%20paper)

* OP-XY Agents spec (showing integration points for metrics and WS)[\[3\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L52-L60)[\[4\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L168-L175)

---

[\[1\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L62-L70) [\[2\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L50-L58) [\[3\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L52-L60) [\[4\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L168-L175) [\[5\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L30-L38) [\[12\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L168-L176) [\[13\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md#L18-L22) AGENTS.md

[https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/AGENTS.md)

[\[6\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/docs/prs/0001-initial-docs.md#L6-L9) [\[35\]](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/docs/prs/0001-initial-docs.md#L26-L34) 0001-initial-docs.md

[https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/docs/prs/0001-initial-docs.md](https://github.com/kmorrill/op-xy-vibing/blob/84757a5faac360c3b048271b38b0d1a323c6e6a8/docs/prs/0001-initial-docs.md)

[\[7\]](https://existential.audio/blackhole/#:~:text=BlackHole%20is%20a%20modern%20macOS,applications%20with%20zero%20additional%20latency) BlackHole: Route Audio Between Apps \- Existential Audio

[https://existential.audio/blackhole/](https://existential.audio/blackhole/)

[\[8\]](https://github.com/spatialaudio/python-sounddevice#:~:text=spatialaudio%2Fpython,see) spatialaudio/python-sounddevice: :sound: Play and Record ... \- GitHub

[https://github.com/spatialaudio/python-sounddevice](https://github.com/spatialaudio/python-sounddevice)

[\[9\]](https://soundcard.readthedocs.io/en/latest/#:~:text=SoundCard%20is%20licensed%20under%20the,clause%20license) [\[10\]](https://soundcard.readthedocs.io/en/latest/#:~:text=import%20soundcard%20as%20sc) [\[11\]](https://soundcard.readthedocs.io/en/latest/#:~:text=,abs%28data%29%29%2C%20samplerate%3D48000) [\[31\]](https://soundcard.readthedocs.io/en/latest/#:~:text=,get_microphone%28%27FS2i2) SoundCard — SoundCard 0.2.0 documentation

[https://soundcard.readthedocs.io/en/latest/](https://soundcard.readthedocs.io/en/latest/)

[\[14\]](https://www.eecs.qmul.ac.uk/~josh/documents/2021/21076.pdf#:~:text=%5BPDF%5D%20Convention%20Paper%2010483%20,is%20both%20fully%20compliant) \[PDF\] Convention Paper 10483 \- Queen Mary University of London

[https://www.eecs.qmul.ac.uk/\~josh/documents/2021/21076.pdf](https://www.eecs.qmul.ac.uk/~josh/documents/2021/21076.pdf)

[\[15\]](https://www.mdpi.com/1999-4893/17/6/228#:~:text=Automated%20Personalized%20Loudness%20Control%20for,both%20human%20perception%20and) Automated Personalized Loudness Control for Multi-Track Recordings

[https://www.mdpi.com/1999-4893/17/6/228](https://www.mdpi.com/1999-4893/17/6/228)

[\[16\]](https://github.com/csteinmetz1/pyloudnorm#:~:text=Implementation%20of%20ITU,weighting%20filters%20for%20additional%20control) [\[17\]](https://github.com/csteinmetz1/pyloudnorm#:~:text=,integrated_loudness%28data) [\[18\]](https://github.com/csteinmetz1/pyloudnorm#:~:text=import%20soundfile%20as%20sf%20import,pyloudnorm%20as%20pyln) [\[19\]](https://github.com/csteinmetz1/pyloudnorm#:~:text=License) GitHub \- csteinmetz1/pyloudnorm: Flexible audio loudness meter in Python with implementation of ITU-R BS.1770-4 loudness algorithm

[https://github.com/csteinmetz1/pyloudnorm](https://github.com/csteinmetz1/pyloudnorm)

[\[20\]](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html#:~:text=Spectral%20Centroid) [\[21\]](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html#:~:text=,the%20%E2%80%9Cbrightness%E2%80%9D%20of%20a%20sound) [\[22\]](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html#:~:text=Let%E2%80%99s%20compute%20and%20visualize%20the,spectral%20centroid) [\[33\]](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html#:~:text=Applications%20of%20Zero%20Crossing%20Rate%3A) Audio Features — The GenAI Guidebook

[https://ravinkumar.com/GenAiGuidebook/audio/audio\_feature\_extraction.html](https://ravinkumar.com/GenAiGuidebook/audio/audio_feature_extraction.html)

[\[23\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=loudness%20,key%20and%20scale%2C%20tuning%20frequency) [\[27\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=analysis%20and%20audio,classifying%20the%20results%20of%20audio) [\[28\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=complexity%2C%20rolloff%2C%20contrast%2C%20HFC%2C%20inharmonicity,annotations%20based%20on%20SVM%20classifiers) [\[32\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=zero,annotations%20based%20on%20SVM%20classifiers) [\[36\]](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/#:~:text=because%20these%20techniques%20allow%20novel,be%20complemented%20with%20Gaia%2C%20a) ESSENTIA: an open source library for audio analysis – ACM SIGMM Records

[https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/](https://records.sigmm.org/2014/03/20/essentia-an-open-source-library-for-audio-analysis/)

[\[24\]](https://www.arpjournal.com/asarpwp/wp-content/uploads/2021/12/Kirsten-Hermes_ARP2019.pdf#:~:text=As%20established%20in%20a%20prior,are%20established%20in%20this%20paper) Kirsten Hermes\_JARP.pdf

[https://www.arpjournal.com/asarpwp/wp-content/uploads/2021/12/Kirsten-Hermes\_ARP2019.pdf](https://www.arpjournal.com/asarpwp/wp-content/uploads/2021/12/Kirsten-Hermes_ARP2019.pdf)

[\[25\]](https://essentia.upf.edu/tutorial_tonal_hpcpkeyscale.html#:~:text=Essentia%20essentia,edma%20%2C%20is%20specifically) Tonality analysis: HPCP, key and scale detection \- Essentia

[https://essentia.upf.edu/tutorial\_tonal\_hpcpkeyscale.html](https://essentia.upf.edu/tutorial_tonal_hpcpkeyscale.html)

[\[26\]](https://mtg.github.io/essentia.js/docs/api/EssentiaExtractor.html#:~:text=EssentiaExtractor%20,estimation%20using%20the%20Key%20algorithm) EssentiaExtractor \- MTG projects

[https://mtg.github.io/essentia.js/docs/api/EssentiaExtractor.html](https://mtg.github.io/essentia.js/docs/api/EssentiaExtractor.html)

[\[29\]](https://github.com/aubio/aubio#:~:text=aubio%20is%20free%20software%3A%20you,the%20Free%20Software%20Foundation%2C) aubio/aubio: a library for audio and music analysis \- GitHub

[https://github.com/aubio/aubio](https://github.com/aubio/aubio)

[\[30\]](https://www.izotope.com/en/learn/what-is-frequency-masking?srsltid=AfmBOoqF27633mIFtuAuALAhpmzAgtqwIB5yZVXiIwNIN2FU4PDGW0cl#:~:text=What%20Is%20Frequency%20Masking%3F%20,in%20the%20same%20general%20location) What Is Frequency Masking? \- iZotope

[https://www.izotope.com/en/learn/what-is-frequency-masking?srsltid=AfmBOoqF27633mIFtuAuALAhpmzAgtqwIB5yZVXiIwNIN2FU4PDGW0cl](https://www.izotope.com/en/learn/what-is-frequency-masking?srsltid=AfmBOoqF27633mIFtuAuALAhpmzAgtqwIB5yZVXiIwNIN2FU4PDGW0cl)

[\[34\]](https://cs229.stanford.edu/proj2019spr/report/3.pdf#:~:text=are%20for%20the%20well%20known,networks%20for%20unsupervised%20audio%20classification) [\[37\]](https://cs229.stanford.edu/proj2019spr/report/3.pdf#:~:text=genre%20with%2064,business%20value%2C%20but%20of%20the) cs229.stanford.edu

[https://cs229.stanford.edu/proj2019spr/report/3.pdf](https://cs229.stanford.edu/proj2019spr/report/3.pdf)